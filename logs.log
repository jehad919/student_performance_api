2025-12-30 10:20:43,584:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:20:43,584:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:20:43,584:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:20:43,584:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:21:57,784:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:21:57,785:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:21:57,785:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:21:57,785:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:37:34,589:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:37:34,590:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:37:34,590:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:37:34,590:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:46:08,061:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:46:08,061:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:46:08,061:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:46:08,061:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:46:09,158:INFO:PyCaret ClassificationExperiment
2025-12-30 10:46:09,158:INFO:Logging name: clf-default-name
2025-12-30 10:46:09,158:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-12-30 10:46:09,159:INFO:version 3.3.2
2025-12-30 10:46:09,159:INFO:Initializing setup()
2025-12-30 10:46:09,159:INFO:self.USI: eb86
2025-12-30 10:46:09,159:INFO:self._variable_keys: {'fix_imbalance', '_ml_usecase', 'exp_id', 'data', 'pipeline', 'gpu_param', 'logging_param', 'is_multiclass', 'memory', 'y_train', 'fold_groups_param', 'fold_generator', 'X_train', '_available_plots', 'target_param', 'y', 'y_test', 'X', 'USI', 'gpu_n_jobs_param', 'log_plots_param', 'fold_shuffle_param', 'idx', 'html_param', 'exp_name_log', 'n_jobs_param', 'seed', 'X_test'}
2025-12-30 10:46:09,159:INFO:Checking environment
2025-12-30 10:46:09,159:INFO:python_version: 3.10.11
2025-12-30 10:46:09,159:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-12-30 10:46:09,159:INFO:machine: AMD64
2025-12-30 10:46:09,169:INFO:platform: Windows-10-10.0.26200-SP0
2025-12-30 10:46:09,169:INFO:Memory: svmem(total=8258220032, available=960634880, percent=88.4, used=7297585152, free=960634880)
2025-12-30 10:46:09,169:INFO:Physical Core: 10
2025-12-30 10:46:09,171:INFO:Logical Core: 12
2025-12-30 10:46:09,171:INFO:Checking libraries
2025-12-30 10:46:09,171:INFO:System:
2025-12-30 10:46:09,171:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-12-30 10:46:09,171:INFO:executable: C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\python.exe
2025-12-30 10:46:09,171:INFO:   machine: Windows-10-10.0.26200-SP0
2025-12-30 10:46:09,172:INFO:PyCaret required dependencies:
2025-12-30 10:46:09,362:INFO:                 pip: 23.0.1
2025-12-30 10:46:09,362:INFO:          setuptools: 65.5.0
2025-12-30 10:46:09,362:INFO:             pycaret: 3.3.2
2025-12-30 10:46:09,362:INFO:             IPython: 8.37.0
2025-12-30 10:46:09,362:INFO:          ipywidgets: 8.1.8
2025-12-30 10:46:09,362:INFO:                tqdm: 4.67.1
2025-12-30 10:46:09,362:INFO:               numpy: 1.26.4
2025-12-30 10:46:09,362:INFO:              pandas: 2.1.4
2025-12-30 10:46:09,362:INFO:              jinja2: 3.1.6
2025-12-30 10:46:09,362:INFO:               scipy: 1.11.4
2025-12-30 10:46:09,362:INFO:              joblib: 1.3.2
2025-12-30 10:46:09,362:INFO:             sklearn: 1.4.2
2025-12-30 10:46:09,362:INFO:                pyod: 2.0.6
2025-12-30 10:46:09,362:INFO:            imblearn: 0.14.1
2025-12-30 10:46:09,362:INFO:   category_encoders: 2.7.0
2025-12-30 10:46:09,362:INFO:            lightgbm: 4.6.0
2025-12-30 10:46:09,362:INFO:               numba: 0.63.1
2025-12-30 10:46:09,362:INFO:            requests: 2.32.5
2025-12-30 10:46:09,362:INFO:          matplotlib: 3.7.5
2025-12-30 10:46:09,362:INFO:          scikitplot: 0.3.7
2025-12-30 10:46:09,362:INFO:         yellowbrick: 1.5
2025-12-30 10:46:09,362:INFO:              plotly: 6.5.0
2025-12-30 10:46:09,362:INFO:    plotly-resampler: Not installed
2025-12-30 10:46:09,362:INFO:             kaleido: 1.2.0
2025-12-30 10:46:09,362:INFO:           schemdraw: 0.15
2025-12-30 10:46:09,362:INFO:         statsmodels: 0.14.6
2025-12-30 10:46:09,362:INFO:              sktime: 0.26.0
2025-12-30 10:46:09,362:INFO:               tbats: 1.1.3
2025-12-30 10:46:09,362:INFO:            pmdarima: 2.0.4
2025-12-30 10:46:09,362:INFO:              psutil: 7.2.1
2025-12-30 10:46:09,362:INFO:          markupsafe: 3.0.3
2025-12-30 10:46:09,362:INFO:             pickle5: Not installed
2025-12-30 10:46:09,362:INFO:         cloudpickle: 3.1.2
2025-12-30 10:46:09,362:INFO:         deprecation: 2.1.0
2025-12-30 10:46:09,362:INFO:              xxhash: 3.6.0
2025-12-30 10:46:09,362:INFO:           wurlitzer: Not installed
2025-12-30 10:46:09,362:INFO:PyCaret optional dependencies:
2025-12-30 10:46:09,367:INFO:                shap: Not installed
2025-12-30 10:46:09,367:INFO:           interpret: Not installed
2025-12-30 10:46:09,367:INFO:                umap: Not installed
2025-12-30 10:46:09,367:INFO:     ydata_profiling: Not installed
2025-12-30 10:46:09,367:INFO:  explainerdashboard: Not installed
2025-12-30 10:46:09,367:INFO:             autoviz: Not installed
2025-12-30 10:46:09,367:INFO:           fairlearn: Not installed
2025-12-30 10:46:09,367:INFO:          deepchecks: Not installed
2025-12-30 10:46:09,367:INFO:             xgboost: Not installed
2025-12-30 10:46:09,367:INFO:            catboost: Not installed
2025-12-30 10:46:09,367:INFO:              kmodes: Not installed
2025-12-30 10:46:09,367:INFO:             mlxtend: Not installed
2025-12-30 10:46:09,367:INFO:       statsforecast: Not installed
2025-12-30 10:46:09,367:INFO:        tune_sklearn: Not installed
2025-12-30 10:46:09,367:INFO:                 ray: Not installed
2025-12-30 10:46:09,367:INFO:            hyperopt: Not installed
2025-12-30 10:46:09,367:INFO:              optuna: Not installed
2025-12-30 10:46:09,367:INFO:               skopt: Not installed
2025-12-30 10:46:09,367:INFO:              mlflow: Not installed
2025-12-30 10:46:09,367:INFO:              gradio: Not installed
2025-12-30 10:46:09,367:INFO:             fastapi: Not installed
2025-12-30 10:46:09,367:INFO:             uvicorn: Not installed
2025-12-30 10:46:09,367:INFO:              m2cgen: Not installed
2025-12-30 10:46:09,367:INFO:           evidently: Not installed
2025-12-30 10:46:09,367:INFO:               fugue: Not installed
2025-12-30 10:46:09,367:INFO:           streamlit: Not installed
2025-12-30 10:46:09,367:INFO:             prophet: Not installed
2025-12-30 10:46:09,367:INFO:None
2025-12-30 10:46:09,367:INFO:Set up data.
2025-12-30 10:46:09,378:INFO:Set up folding strategy.
2025-12-30 10:46:09,378:INFO:Set up train/test split.
2025-12-30 10:46:09,383:INFO:Set up index.
2025-12-30 10:46:09,384:INFO:Assigning column types.
2025-12-30 10:46:09,386:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-12-30 10:46:09,411:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 10:46:09,417:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:46:09,445:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,445:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,471:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 10:46:09,472:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:46:09,491:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,491:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,491:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-12-30 10:46:09,521:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:46:09,540:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,540:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,573:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:46:09,593:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,593:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,594:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-12-30 10:46:09,649:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,649:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,697:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,697:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:09,701:INFO:Preparing preprocessing pipeline...
2025-12-30 10:46:09,702:INFO:Set up simple imputation.
2025-12-30 10:46:09,704:INFO:Set up encoding of ordinal features.
2025-12-30 10:46:09,709:INFO:Set up encoding of categorical features.
2025-12-30 10:46:09,789:INFO:Finished creating preprocessing pipeline.
2025-12-30 10:46:09,851:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-12-30 10:46:09,852:INFO:Creating final display dataframe.
2025-12-30 10:46:10,031:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 25)
5   Transformed train set shape        (1674, 25)
6    Transformed test set shape         (718, 25)
7               Ignore features                 1
8              Numeric features                 3
9          Categorical features                 9
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              eb86
2025-12-30 10:46:10,079:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:10,079:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:10,124:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:10,124:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:46:10,124:INFO:setup() successfully completed in 0.97s...............
2025-12-30 10:46:10,125:INFO:Initializing compare_models()
2025-12-30 10:46:10,125:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-12-30 10:46:10,125:INFO:Checking exceptions
2025-12-30 10:46:10,129:INFO:Preparing display monitor
2025-12-30 10:46:10,132:INFO:Initializing Logistic Regression
2025-12-30 10:46:10,132:INFO:Total runtime is 0.0 minutes
2025-12-30 10:46:10,132:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:10,132:INFO:Initializing create_model()
2025-12-30 10:46:10,132:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:10,132:INFO:Checking exceptions
2025-12-30 10:46:10,132:INFO:Importing libraries
2025-12-30 10:46:10,132:INFO:Copying training dataset
2025-12-30 10:46:10,136:INFO:Defining folds
2025-12-30 10:46:10,136:INFO:Declaring metric variables
2025-12-30 10:46:10,136:INFO:Importing untrained model
2025-12-30 10:46:10,136:INFO:Logistic Regression Imported successfully
2025-12-30 10:46:10,136:INFO:Starting cross validation
2025-12-30 10:46:10,137:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:18,488:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:46:18,498:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:46:18,510:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:46:18,518:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:46:18,532:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:46:18,536:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:46:18,600:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:46:18,604:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:46:18,661:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:18,662:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:18,662:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:18,662:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:18,692:INFO:Calculating mean and std
2025-12-30 10:46:18,697:INFO:Creating metrics dataframe
2025-12-30 10:46:18,704:INFO:Uploading results into container
2025-12-30 10:46:18,707:INFO:Uploading model into container now
2025-12-30 10:46:18,710:INFO:_master_model_container: 1
2025-12-30 10:46:18,711:INFO:_display_container: 2
2025-12-30 10:46:18,713:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 10:46:18,713:INFO:create_model() successfully completed......................................
2025-12-30 10:46:18,980:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:18,980:INFO:Creating metrics dataframe
2025-12-30 10:46:18,982:INFO:Initializing K Neighbors Classifier
2025-12-30 10:46:18,982:INFO:Total runtime is 0.14748497009277345 minutes
2025-12-30 10:46:18,982:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:18,982:INFO:Initializing create_model()
2025-12-30 10:46:18,983:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:18,983:INFO:Checking exceptions
2025-12-30 10:46:18,983:INFO:Importing libraries
2025-12-30 10:46:18,983:INFO:Copying training dataset
2025-12-30 10:46:18,985:INFO:Defining folds
2025-12-30 10:46:18,985:INFO:Declaring metric variables
2025-12-30 10:46:18,985:INFO:Importing untrained model
2025-12-30 10:46:18,985:INFO:K Neighbors Classifier Imported successfully
2025-12-30 10:46:18,986:INFO:Starting cross validation
2025-12-30 10:46:18,987:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:21,149:INFO:Calculating mean and std
2025-12-30 10:46:21,151:INFO:Creating metrics dataframe
2025-12-30 10:46:21,154:INFO:Uploading results into container
2025-12-30 10:46:21,155:INFO:Uploading model into container now
2025-12-30 10:46:21,156:INFO:_master_model_container: 2
2025-12-30 10:46:21,156:INFO:_display_container: 2
2025-12-30 10:46:21,156:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-12-30 10:46:21,156:INFO:create_model() successfully completed......................................
2025-12-30 10:46:21,243:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:21,243:INFO:Creating metrics dataframe
2025-12-30 10:46:21,246:INFO:Initializing Naive Bayes
2025-12-30 10:46:21,246:INFO:Total runtime is 0.18522584040959678 minutes
2025-12-30 10:46:21,246:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:21,246:INFO:Initializing create_model()
2025-12-30 10:46:21,246:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:21,246:INFO:Checking exceptions
2025-12-30 10:46:21,246:INFO:Importing libraries
2025-12-30 10:46:21,246:INFO:Copying training dataset
2025-12-30 10:46:21,249:INFO:Defining folds
2025-12-30 10:46:21,249:INFO:Declaring metric variables
2025-12-30 10:46:21,249:INFO:Importing untrained model
2025-12-30 10:46:21,250:INFO:Naive Bayes Imported successfully
2025-12-30 10:46:21,250:INFO:Starting cross validation
2025-12-30 10:46:21,251:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:21,574:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:21,575:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:21,575:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:21,586:INFO:Calculating mean and std
2025-12-30 10:46:21,587:INFO:Creating metrics dataframe
2025-12-30 10:46:21,588:INFO:Uploading results into container
2025-12-30 10:46:21,590:INFO:Uploading model into container now
2025-12-30 10:46:21,590:INFO:_master_model_container: 3
2025-12-30 10:46:21,590:INFO:_display_container: 2
2025-12-30 10:46:21,590:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-12-30 10:46:21,590:INFO:create_model() successfully completed......................................
2025-12-30 10:46:21,661:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:21,661:INFO:Creating metrics dataframe
2025-12-30 10:46:21,663:INFO:Initializing Decision Tree Classifier
2025-12-30 10:46:21,663:INFO:Total runtime is 0.19217284520467123 minutes
2025-12-30 10:46:21,663:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:21,663:INFO:Initializing create_model()
2025-12-30 10:46:21,663:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:21,663:INFO:Checking exceptions
2025-12-30 10:46:21,663:INFO:Importing libraries
2025-12-30 10:46:21,663:INFO:Copying training dataset
2025-12-30 10:46:21,666:INFO:Defining folds
2025-12-30 10:46:21,666:INFO:Declaring metric variables
2025-12-30 10:46:21,666:INFO:Importing untrained model
2025-12-30 10:46:21,666:INFO:Decision Tree Classifier Imported successfully
2025-12-30 10:46:21,666:INFO:Starting cross validation
2025-12-30 10:46:21,668:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:21,994:INFO:Calculating mean and std
2025-12-30 10:46:21,995:INFO:Creating metrics dataframe
2025-12-30 10:46:21,996:INFO:Uploading results into container
2025-12-30 10:46:21,996:INFO:Uploading model into container now
2025-12-30 10:46:21,997:INFO:_master_model_container: 4
2025-12-30 10:46:21,997:INFO:_display_container: 2
2025-12-30 10:46:21,997:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-12-30 10:46:21,997:INFO:create_model() successfully completed......................................
2025-12-30 10:46:22,066:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:22,066:INFO:Creating metrics dataframe
2025-12-30 10:46:22,069:INFO:Initializing SVM - Linear Kernel
2025-12-30 10:46:22,069:INFO:Total runtime is 0.19893681208292643 minutes
2025-12-30 10:46:22,069:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:22,069:INFO:Initializing create_model()
2025-12-30 10:46:22,069:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:22,069:INFO:Checking exceptions
2025-12-30 10:46:22,069:INFO:Importing libraries
2025-12-30 10:46:22,069:INFO:Copying training dataset
2025-12-30 10:46:22,072:INFO:Defining folds
2025-12-30 10:46:22,072:INFO:Declaring metric variables
2025-12-30 10:46:22,072:INFO:Importing untrained model
2025-12-30 10:46:22,072:INFO:SVM - Linear Kernel Imported successfully
2025-12-30 10:46:22,073:INFO:Starting cross validation
2025-12-30 10:46:22,074:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:22,463:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,470:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,472:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,474:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,479:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,481:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,492:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,494:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,498:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,501:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,503:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,503:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,504:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,505:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,506:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,509:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,511:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,513:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,527:INFO:Calculating mean and std
2025-12-30 10:46:22,528:INFO:Creating metrics dataframe
2025-12-30 10:46:22,529:INFO:Uploading results into container
2025-12-30 10:46:22,530:INFO:Uploading model into container now
2025-12-30 10:46:22,530:INFO:_master_model_container: 5
2025-12-30 10:46:22,530:INFO:_display_container: 2
2025-12-30 10:46:22,531:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-12-30 10:46:22,531:INFO:create_model() successfully completed......................................
2025-12-30 10:46:22,590:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:22,590:INFO:Creating metrics dataframe
2025-12-30 10:46:22,592:INFO:Initializing Ridge Classifier
2025-12-30 10:46:22,592:INFO:Total runtime is 0.2076613982518514 minutes
2025-12-30 10:46:22,592:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:22,592:INFO:Initializing create_model()
2025-12-30 10:46:22,592:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:22,592:INFO:Checking exceptions
2025-12-30 10:46:22,592:INFO:Importing libraries
2025-12-30 10:46:22,592:INFO:Copying training dataset
2025-12-30 10:46:22,595:INFO:Defining folds
2025-12-30 10:46:22,595:INFO:Declaring metric variables
2025-12-30 10:46:22,595:INFO:Importing untrained model
2025-12-30 10:46:22,595:INFO:Ridge Classifier Imported successfully
2025-12-30 10:46:22,595:INFO:Starting cross validation
2025-12-30 10:46:22,596:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:22,845:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,850:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,853:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,858:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,861:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,863:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,866:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,868:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,868:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,870:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,873:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,875:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,881:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,884:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,885:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,887:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,890:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,890:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,892:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:22,896:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:22,901:INFO:Calculating mean and std
2025-12-30 10:46:22,901:INFO:Creating metrics dataframe
2025-12-30 10:46:22,903:INFO:Uploading results into container
2025-12-30 10:46:22,903:INFO:Uploading model into container now
2025-12-30 10:46:22,904:INFO:_master_model_container: 6
2025-12-30 10:46:22,904:INFO:_display_container: 2
2025-12-30 10:46:22,904:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-12-30 10:46:22,904:INFO:create_model() successfully completed......................................
2025-12-30 10:46:22,967:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:22,967:INFO:Creating metrics dataframe
2025-12-30 10:46:22,968:INFO:Initializing Random Forest Classifier
2025-12-30 10:46:22,968:INFO:Total runtime is 0.21391932566960653 minutes
2025-12-30 10:46:22,969:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:22,969:INFO:Initializing create_model()
2025-12-30 10:46:22,969:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:22,969:INFO:Checking exceptions
2025-12-30 10:46:22,969:INFO:Importing libraries
2025-12-30 10:46:22,969:INFO:Copying training dataset
2025-12-30 10:46:22,971:INFO:Defining folds
2025-12-30 10:46:22,971:INFO:Declaring metric variables
2025-12-30 10:46:22,971:INFO:Importing untrained model
2025-12-30 10:46:22,971:INFO:Random Forest Classifier Imported successfully
2025-12-30 10:46:22,971:INFO:Starting cross validation
2025-12-30 10:46:22,972:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:23,760:INFO:Calculating mean and std
2025-12-30 10:46:23,761:INFO:Creating metrics dataframe
2025-12-30 10:46:23,762:INFO:Uploading results into container
2025-12-30 10:46:23,762:INFO:Uploading model into container now
2025-12-30 10:46:23,763:INFO:_master_model_container: 7
2025-12-30 10:46:23,763:INFO:_display_container: 2
2025-12-30 10:46:23,763:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-12-30 10:46:23,763:INFO:create_model() successfully completed......................................
2025-12-30 10:46:23,823:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:23,823:INFO:Creating metrics dataframe
2025-12-30 10:46:23,824:INFO:Initializing Quadratic Discriminant Analysis
2025-12-30 10:46:23,824:INFO:Total runtime is 0.2281962275505066 minutes
2025-12-30 10:46:23,824:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:23,825:INFO:Initializing create_model()
2025-12-30 10:46:23,825:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:23,825:INFO:Checking exceptions
2025-12-30 10:46:23,825:INFO:Importing libraries
2025-12-30 10:46:23,825:INFO:Copying training dataset
2025-12-30 10:46:23,827:INFO:Defining folds
2025-12-30 10:46:23,827:INFO:Declaring metric variables
2025-12-30 10:46:23,827:INFO:Importing untrained model
2025-12-30 10:46:23,828:INFO:Quadratic Discriminant Analysis Imported successfully
2025-12-30 10:46:23,828:INFO:Starting cross validation
2025-12-30 10:46:23,829:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:24,063:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:46:24,063:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:46:24,064:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:46:24,064:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:46:24,064:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:46:24,064:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:46:24,124:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,135:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,137:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,139:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,141:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,144:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,147:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,170:INFO:Calculating mean and std
2025-12-30 10:46:24,171:INFO:Creating metrics dataframe
2025-12-30 10:46:24,172:INFO:Uploading results into container
2025-12-30 10:46:24,172:INFO:Uploading model into container now
2025-12-30 10:46:24,173:INFO:_master_model_container: 8
2025-12-30 10:46:24,173:INFO:_display_container: 2
2025-12-30 10:46:24,173:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-12-30 10:46:24,173:INFO:create_model() successfully completed......................................
2025-12-30 10:46:24,232:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:24,232:INFO:Creating metrics dataframe
2025-12-30 10:46:24,234:INFO:Initializing Ada Boost Classifier
2025-12-30 10:46:24,234:INFO:Total runtime is 0.2350313385327657 minutes
2025-12-30 10:46:24,234:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:24,234:INFO:Initializing create_model()
2025-12-30 10:46:24,234:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:24,235:INFO:Checking exceptions
2025-12-30 10:46:24,235:INFO:Importing libraries
2025-12-30 10:46:24,235:INFO:Copying training dataset
2025-12-30 10:46:24,237:INFO:Defining folds
2025-12-30 10:46:24,237:INFO:Declaring metric variables
2025-12-30 10:46:24,237:INFO:Importing untrained model
2025-12-30 10:46:24,237:INFO:Ada Boost Classifier Imported successfully
2025-12-30 10:46:24,237:INFO:Starting cross validation
2025-12-30 10:46:24,238:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:24,479:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:46:24,479:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:46:24,479:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:46:24,498:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:46:24,831:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,833:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,834:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,837:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,841:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,842:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,843:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,845:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,849:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,856:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:24,871:INFO:Calculating mean and std
2025-12-30 10:46:24,872:INFO:Creating metrics dataframe
2025-12-30 10:46:24,873:INFO:Uploading results into container
2025-12-30 10:46:24,874:INFO:Uploading model into container now
2025-12-30 10:46:24,874:INFO:_master_model_container: 9
2025-12-30 10:46:24,874:INFO:_display_container: 2
2025-12-30 10:46:24,874:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-12-30 10:46:24,875:INFO:create_model() successfully completed......................................
2025-12-30 10:46:24,936:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:24,936:INFO:Creating metrics dataframe
2025-12-30 10:46:24,938:INFO:Initializing Gradient Boosting Classifier
2025-12-30 10:46:24,938:INFO:Total runtime is 0.24676329294840493 minutes
2025-12-30 10:46:24,938:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:24,938:INFO:Initializing create_model()
2025-12-30 10:46:24,938:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:24,938:INFO:Checking exceptions
2025-12-30 10:46:24,938:INFO:Importing libraries
2025-12-30 10:46:24,938:INFO:Copying training dataset
2025-12-30 10:46:24,942:INFO:Defining folds
2025-12-30 10:46:24,942:INFO:Declaring metric variables
2025-12-30 10:46:24,942:INFO:Importing untrained model
2025-12-30 10:46:24,942:INFO:Gradient Boosting Classifier Imported successfully
2025-12-30 10:46:24,942:INFO:Starting cross validation
2025-12-30 10:46:24,943:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:27,106:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,118:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,135:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,138:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,141:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,145:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,148:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,173:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,187:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,188:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,207:INFO:Calculating mean and std
2025-12-30 10:46:27,207:INFO:Creating metrics dataframe
2025-12-30 10:46:27,209:INFO:Uploading results into container
2025-12-30 10:46:27,209:INFO:Uploading model into container now
2025-12-30 10:46:27,209:INFO:_master_model_container: 10
2025-12-30 10:46:27,209:INFO:_display_container: 2
2025-12-30 10:46:27,210:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-12-30 10:46:27,210:INFO:create_model() successfully completed......................................
2025-12-30 10:46:27,263:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:27,263:INFO:Creating metrics dataframe
2025-12-30 10:46:27,264:INFO:Initializing Linear Discriminant Analysis
2025-12-30 10:46:27,264:INFO:Total runtime is 0.2855225086212158 minutes
2025-12-30 10:46:27,264:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:27,264:INFO:Initializing create_model()
2025-12-30 10:46:27,264:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:27,264:INFO:Checking exceptions
2025-12-30 10:46:27,264:INFO:Importing libraries
2025-12-30 10:46:27,264:INFO:Copying training dataset
2025-12-30 10:46:27,266:INFO:Defining folds
2025-12-30 10:46:27,266:INFO:Declaring metric variables
2025-12-30 10:46:27,267:INFO:Importing untrained model
2025-12-30 10:46:27,267:INFO:Linear Discriminant Analysis Imported successfully
2025-12-30 10:46:27,267:INFO:Starting cross validation
2025-12-30 10:46:27,268:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:27,514:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,521:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,522:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,522:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,524:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,524:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,526:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,528:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:27,531:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:46:27,552:INFO:Calculating mean and std
2025-12-30 10:46:27,552:INFO:Creating metrics dataframe
2025-12-30 10:46:27,553:INFO:Uploading results into container
2025-12-30 10:46:27,554:INFO:Uploading model into container now
2025-12-30 10:46:27,554:INFO:_master_model_container: 11
2025-12-30 10:46:27,554:INFO:_display_container: 2
2025-12-30 10:46:27,554:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-12-30 10:46:27,554:INFO:create_model() successfully completed......................................
2025-12-30 10:46:27,607:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:27,607:INFO:Creating metrics dataframe
2025-12-30 10:46:27,608:INFO:Initializing Extra Trees Classifier
2025-12-30 10:46:27,608:INFO:Total runtime is 0.2912613789240519 minutes
2025-12-30 10:46:27,608:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:27,609:INFO:Initializing create_model()
2025-12-30 10:46:27,609:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:27,609:INFO:Checking exceptions
2025-12-30 10:46:27,609:INFO:Importing libraries
2025-12-30 10:46:27,609:INFO:Copying training dataset
2025-12-30 10:46:27,611:INFO:Defining folds
2025-12-30 10:46:27,611:INFO:Declaring metric variables
2025-12-30 10:46:27,611:INFO:Importing untrained model
2025-12-30 10:46:27,611:INFO:Extra Trees Classifier Imported successfully
2025-12-30 10:46:27,611:INFO:Starting cross validation
2025-12-30 10:46:27,613:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:28,306:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:28,348:INFO:Calculating mean and std
2025-12-30 10:46:28,349:INFO:Creating metrics dataframe
2025-12-30 10:46:28,351:INFO:Uploading results into container
2025-12-30 10:46:28,352:INFO:Uploading model into container now
2025-12-30 10:46:28,352:INFO:_master_model_container: 12
2025-12-30 10:46:28,353:INFO:_display_container: 2
2025-12-30 10:46:28,353:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-12-30 10:46:28,353:INFO:create_model() successfully completed......................................
2025-12-30 10:46:28,419:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:28,419:INFO:Creating metrics dataframe
2025-12-30 10:46:28,421:INFO:Initializing Light Gradient Boosting Machine
2025-12-30 10:46:28,421:INFO:Total runtime is 0.304804523785909 minutes
2025-12-30 10:46:28,421:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:28,421:INFO:Initializing create_model()
2025-12-30 10:46:28,421:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:28,422:INFO:Checking exceptions
2025-12-30 10:46:28,422:INFO:Importing libraries
2025-12-30 10:46:28,422:INFO:Copying training dataset
2025-12-30 10:46:28,424:INFO:Defining folds
2025-12-30 10:46:28,424:INFO:Declaring metric variables
2025-12-30 10:46:28,424:INFO:Importing untrained model
2025-12-30 10:46:28,425:INFO:Light Gradient Boosting Machine Imported successfully
2025-12-30 10:46:28,425:INFO:Starting cross validation
2025-12-30 10:46:28,426:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:33,258:INFO:Calculating mean and std
2025-12-30 10:46:33,259:INFO:Creating metrics dataframe
2025-12-30 10:46:33,261:INFO:Uploading results into container
2025-12-30 10:46:33,261:INFO:Uploading model into container now
2025-12-30 10:46:33,261:INFO:_master_model_container: 13
2025-12-30 10:46:33,261:INFO:_display_container: 2
2025-12-30 10:46:33,262:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-12-30 10:46:33,262:INFO:create_model() successfully completed......................................
2025-12-30 10:46:33,338:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:33,338:INFO:Creating metrics dataframe
2025-12-30 10:46:33,341:INFO:Initializing Dummy Classifier
2025-12-30 10:46:33,341:INFO:Total runtime is 0.3868142088254293 minutes
2025-12-30 10:46:33,341:INFO:SubProcess create_model() called ==================================
2025-12-30 10:46:33,342:INFO:Initializing create_model()
2025-12-30 10:46:33,342:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000161B5DFBA60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:33,342:INFO:Checking exceptions
2025-12-30 10:46:33,342:INFO:Importing libraries
2025-12-30 10:46:33,342:INFO:Copying training dataset
2025-12-30 10:46:33,346:INFO:Defining folds
2025-12-30 10:46:33,346:INFO:Declaring metric variables
2025-12-30 10:46:33,346:INFO:Importing untrained model
2025-12-30 10:46:33,346:INFO:Dummy Classifier Imported successfully
2025-12-30 10:46:33,347:INFO:Starting cross validation
2025-12-30 10:46:33,348:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:46:33,656:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:33,658:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:33,658:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:33,669:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:33,672:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:33,674:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:33,676:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:33,689:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:33,690:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:46:33,696:INFO:Calculating mean and std
2025-12-30 10:46:33,696:INFO:Creating metrics dataframe
2025-12-30 10:46:33,698:INFO:Uploading results into container
2025-12-30 10:46:33,698:INFO:Uploading model into container now
2025-12-30 10:46:33,698:INFO:_master_model_container: 14
2025-12-30 10:46:33,699:INFO:_display_container: 2
2025-12-30 10:46:33,699:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-12-30 10:46:33,699:INFO:create_model() successfully completed......................................
2025-12-30 10:46:33,757:INFO:SubProcess create_model() end ==================================
2025-12-30 10:46:33,758:INFO:Creating metrics dataframe
2025-12-30 10:46:33,761:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-12-30 10:46:33,762:INFO:Initializing create_model()
2025-12-30 10:46:33,762:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000161975E9A20>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:46:33,762:INFO:Checking exceptions
2025-12-30 10:46:33,763:INFO:Importing libraries
2025-12-30 10:46:33,763:INFO:Copying training dataset
2025-12-30 10:46:33,765:INFO:Defining folds
2025-12-30 10:46:33,765:INFO:Declaring metric variables
2025-12-30 10:46:33,765:INFO:Importing untrained model
2025-12-30 10:46:33,765:INFO:Declaring custom model
2025-12-30 10:46:33,765:INFO:Gradient Boosting Classifier Imported successfully
2025-12-30 10:46:33,766:INFO:Cross validation set to False
2025-12-30 10:46:33,766:INFO:Fitting Model
2025-12-30 10:46:34,945:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-12-30 10:46:34,946:INFO:create_model() successfully completed......................................
2025-12-30 10:46:35,017:INFO:_master_model_container: 14
2025-12-30 10:46:35,017:INFO:_display_container: 2
2025-12-30 10:46:35,017:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-12-30 10:46:35,017:INFO:compare_models() successfully completed......................................
2025-12-30 10:46:35,077:INFO:Initializing save_model()
2025-12-30 10:46:35,077:INFO:save_model(model=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), model_name=student_performance_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-12-30 10:46:35,077:INFO:Adding model into prep_pipe
2025-12-30 10:46:35,096:INFO:student_performance_model.pkl saved in current working directory
2025-12-30 10:46:35,164:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Gender...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=123, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-12-30 10:46:35,164:INFO:save_model() successfully completed......................................
2025-12-30 10:52:06,653:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:52:06,654:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:52:06,654:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:52:06,654:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:52:07,744:INFO:PyCaret ClassificationExperiment
2025-12-30 10:52:07,744:INFO:Logging name: clf-default-name
2025-12-30 10:52:07,746:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-12-30 10:52:07,746:INFO:version 3.3.2
2025-12-30 10:52:07,746:INFO:Initializing setup()
2025-12-30 10:52:07,746:INFO:self.USI: e271
2025-12-30 10:52:07,746:INFO:self._variable_keys: {'y', 'idx', 'logging_param', 'gpu_param', 'X_test', 'seed', 'exp_id', 'fold_groups_param', '_ml_usecase', 'y_train', 'X', 'html_param', 'fold_shuffle_param', 'memory', 'USI', '_available_plots', 'fold_generator', 'target_param', 'is_multiclass', 'y_test', 'fix_imbalance', 'pipeline', 'exp_name_log', 'gpu_n_jobs_param', 'data', 'log_plots_param', 'X_train', 'n_jobs_param'}
2025-12-30 10:52:07,746:INFO:Checking environment
2025-12-30 10:52:07,746:INFO:python_version: 3.10.11
2025-12-30 10:52:07,746:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-12-30 10:52:07,746:INFO:machine: AMD64
2025-12-30 10:52:07,794:INFO:platform: Windows-10-10.0.26200-SP0
2025-12-30 10:52:07,795:INFO:Memory: svmem(total=8258220032, available=1293307904, percent=84.3, used=6964912128, free=1293307904)
2025-12-30 10:52:07,795:INFO:Physical Core: 10
2025-12-30 10:52:07,795:INFO:Logical Core: 12
2025-12-30 10:52:07,795:INFO:Checking libraries
2025-12-30 10:52:07,795:INFO:System:
2025-12-30 10:52:07,795:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-12-30 10:52:07,795:INFO:executable: C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\python.exe
2025-12-30 10:52:07,795:INFO:   machine: Windows-10-10.0.26200-SP0
2025-12-30 10:52:07,795:INFO:PyCaret required dependencies:
2025-12-30 10:52:07,816:INFO:                 pip: 23.0.1
2025-12-30 10:52:07,816:INFO:          setuptools: 65.5.0
2025-12-30 10:52:07,816:INFO:             pycaret: 3.3.2
2025-12-30 10:52:07,816:INFO:             IPython: 8.37.0
2025-12-30 10:52:07,816:INFO:          ipywidgets: 8.1.8
2025-12-30 10:52:07,816:INFO:                tqdm: 4.67.1
2025-12-30 10:52:07,816:INFO:               numpy: 1.26.4
2025-12-30 10:52:07,816:INFO:              pandas: 2.1.4
2025-12-30 10:52:07,816:INFO:              jinja2: 3.1.6
2025-12-30 10:52:07,816:INFO:               scipy: 1.11.4
2025-12-30 10:52:07,816:INFO:              joblib: 1.3.2
2025-12-30 10:52:07,816:INFO:             sklearn: 1.4.2
2025-12-30 10:52:07,816:INFO:                pyod: 2.0.6
2025-12-30 10:52:07,816:INFO:            imblearn: 0.14.1
2025-12-30 10:52:07,816:INFO:   category_encoders: 2.7.0
2025-12-30 10:52:07,816:INFO:            lightgbm: 4.6.0
2025-12-30 10:52:07,816:INFO:               numba: 0.63.1
2025-12-30 10:52:07,816:INFO:            requests: 2.32.5
2025-12-30 10:52:07,816:INFO:          matplotlib: 3.7.5
2025-12-30 10:52:07,816:INFO:          scikitplot: 0.3.7
2025-12-30 10:52:07,816:INFO:         yellowbrick: 1.5
2025-12-30 10:52:07,816:INFO:              plotly: 6.5.0
2025-12-30 10:52:07,816:INFO:    plotly-resampler: Not installed
2025-12-30 10:52:07,816:INFO:             kaleido: 1.2.0
2025-12-30 10:52:07,816:INFO:           schemdraw: 0.15
2025-12-30 10:52:07,816:INFO:         statsmodels: 0.14.6
2025-12-30 10:52:07,816:INFO:              sktime: 0.26.0
2025-12-30 10:52:07,816:INFO:               tbats: 1.1.3
2025-12-30 10:52:07,816:INFO:            pmdarima: 2.0.4
2025-12-30 10:52:07,816:INFO:              psutil: 7.2.1
2025-12-30 10:52:07,816:INFO:          markupsafe: 3.0.3
2025-12-30 10:52:07,816:INFO:             pickle5: Not installed
2025-12-30 10:52:07,816:INFO:         cloudpickle: 3.1.2
2025-12-30 10:52:07,816:INFO:         deprecation: 2.1.0
2025-12-30 10:52:07,816:INFO:              xxhash: 3.6.0
2025-12-30 10:52:07,816:INFO:           wurlitzer: Not installed
2025-12-30 10:52:07,816:INFO:PyCaret optional dependencies:
2025-12-30 10:52:07,820:INFO:                shap: Not installed
2025-12-30 10:52:07,820:INFO:           interpret: Not installed
2025-12-30 10:52:07,820:INFO:                umap: Not installed
2025-12-30 10:52:07,820:INFO:     ydata_profiling: Not installed
2025-12-30 10:52:07,820:INFO:  explainerdashboard: Not installed
2025-12-30 10:52:07,820:INFO:             autoviz: Not installed
2025-12-30 10:52:07,820:INFO:           fairlearn: Not installed
2025-12-30 10:52:07,820:INFO:          deepchecks: Not installed
2025-12-30 10:52:07,820:INFO:             xgboost: Not installed
2025-12-30 10:52:07,820:INFO:            catboost: Not installed
2025-12-30 10:52:07,820:INFO:              kmodes: Not installed
2025-12-30 10:52:07,820:INFO:             mlxtend: Not installed
2025-12-30 10:52:07,820:INFO:       statsforecast: Not installed
2025-12-30 10:52:07,820:INFO:        tune_sklearn: Not installed
2025-12-30 10:52:07,820:INFO:                 ray: Not installed
2025-12-30 10:52:07,820:INFO:            hyperopt: Not installed
2025-12-30 10:52:07,821:INFO:              optuna: Not installed
2025-12-30 10:52:07,821:INFO:               skopt: Not installed
2025-12-30 10:52:07,821:INFO:              mlflow: Not installed
2025-12-30 10:52:07,821:INFO:              gradio: Not installed
2025-12-30 10:52:07,821:INFO:             fastapi: Not installed
2025-12-30 10:52:07,821:INFO:             uvicorn: Not installed
2025-12-30 10:52:07,821:INFO:              m2cgen: Not installed
2025-12-30 10:52:07,821:INFO:           evidently: Not installed
2025-12-30 10:52:07,821:INFO:               fugue: Not installed
2025-12-30 10:52:07,821:INFO:           streamlit: Not installed
2025-12-30 10:52:07,821:INFO:             prophet: Not installed
2025-12-30 10:52:07,821:INFO:None
2025-12-30 10:52:07,821:INFO:Set up data.
2025-12-30 10:52:07,829:INFO:Set up folding strategy.
2025-12-30 10:52:07,829:INFO:Set up train/test split.
2025-12-30 10:52:07,835:INFO:Set up index.
2025-12-30 10:52:07,835:INFO:Assigning column types.
2025-12-30 10:52:07,837:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-12-30 10:52:07,862:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 10:52:07,866:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:52:07,891:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:07,891:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:07,920:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 10:52:07,920:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:52:07,938:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:07,938:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:07,938:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-12-30 10:52:07,967:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:52:07,984:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:07,985:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,014:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:52:08,032:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,032:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,032:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-12-30 10:52:08,079:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,080:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,126:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,126:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,129:INFO:Preparing preprocessing pipeline...
2025-12-30 10:52:08,131:INFO:Set up simple imputation.
2025-12-30 10:52:08,133:INFO:Set up encoding of ordinal features.
2025-12-30 10:52:08,138:INFO:Set up encoding of categorical features.
2025-12-30 10:52:08,251:INFO:Finished creating preprocessing pipeline.
2025-12-30 10:52:08,317:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-12-30 10:52:08,318:INFO:Creating final display dataframe.
2025-12-30 10:52:08,526:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 25)
5   Transformed train set shape        (1674, 25)
6    Transformed test set shape         (718, 25)
7               Ignore features                 1
8              Numeric features                 3
9          Categorical features                 9
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              e271
2025-12-30 10:52:08,576:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,576:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,621:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:08,621:INFO:setup() successfully completed in 0.88s...............
2025-12-30 10:52:08,622:INFO:Initializing compare_models()
2025-12-30 10:52:08,622:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000269BD689A20>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000269BD689A20>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-12-30 10:52:08,622:INFO:Checking exceptions
2025-12-30 10:52:08,626:INFO:Preparing display monitor
2025-12-30 10:52:08,630:INFO:Initializing Logistic Regression
2025-12-30 10:52:08,630:INFO:Total runtime is 0.0 minutes
2025-12-30 10:52:08,630:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:08,630:INFO:Initializing create_model()
2025-12-30 10:52:08,630:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000269BD689A20>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000269DBE8BF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:08,630:INFO:Checking exceptions
2025-12-30 10:52:08,630:INFO:Importing libraries
2025-12-30 10:52:08,630:INFO:Copying training dataset
2025-12-30 10:52:08,633:INFO:Defining folds
2025-12-30 10:52:08,633:INFO:Declaring metric variables
2025-12-30 10:52:08,633:INFO:Importing untrained model
2025-12-30 10:52:08,633:INFO:Logistic Regression Imported successfully
2025-12-30 10:52:08,633:INFO:Starting cross validation
2025-12-30 10:52:08,635:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:16,745:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:16,764:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:16,807:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:16,808:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:16,824:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:16,867:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:16,918:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:16,923:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:16,936:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:16,957:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:16,988:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:16,993:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:17,000:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:17,022:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:17,040:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:17,087:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:17,120:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:17,120:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:17,163:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:17,180:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:17,198:INFO:Calculating mean and std
2025-12-30 10:52:17,199:INFO:Creating metrics dataframe
2025-12-30 10:52:17,203:INFO:Uploading results into container
2025-12-30 10:52:17,203:INFO:Uploading model into container now
2025-12-30 10:52:17,205:INFO:_master_model_container: 1
2025-12-30 10:52:17,205:INFO:_display_container: 2
2025-12-30 10:52:17,206:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 10:52:17,206:INFO:create_model() successfully completed......................................
2025-12-30 10:52:17,310:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:17,310:INFO:Creating metrics dataframe
2025-12-30 10:52:17,313:INFO:Initializing K Neighbors Classifier
2025-12-30 10:52:17,314:INFO:Total runtime is 0.14473416010538737 minutes
2025-12-30 10:52:17,314:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:17,314:INFO:Initializing create_model()
2025-12-30 10:52:17,314:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000269BD689A20>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000269DBE8BF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:17,314:INFO:Checking exceptions
2025-12-30 10:52:17,314:INFO:Importing libraries
2025-12-30 10:52:17,314:INFO:Copying training dataset
2025-12-30 10:52:17,317:INFO:Defining folds
2025-12-30 10:52:17,317:INFO:Declaring metric variables
2025-12-30 10:52:17,317:INFO:Importing untrained model
2025-12-30 10:52:17,318:INFO:K Neighbors Classifier Imported successfully
2025-12-30 10:52:17,318:INFO:Starting cross validation
2025-12-30 10:52:17,320:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:20,359:INFO:Calculating mean and std
2025-12-30 10:52:20,361:INFO:Creating metrics dataframe
2025-12-30 10:52:20,363:INFO:Uploading results into container
2025-12-30 10:52:20,364:INFO:Uploading model into container now
2025-12-30 10:52:20,364:INFO:_master_model_container: 2
2025-12-30 10:52:20,365:INFO:_display_container: 2
2025-12-30 10:52:20,365:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-12-30 10:52:20,365:INFO:create_model() successfully completed......................................
2025-12-30 10:52:20,452:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:20,452:INFO:Creating metrics dataframe
2025-12-30 10:52:20,454:INFO:Initializing Naive Bayes
2025-12-30 10:52:20,454:INFO:Total runtime is 0.19707585175832112 minutes
2025-12-30 10:52:20,456:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:20,456:INFO:Initializing create_model()
2025-12-30 10:52:20,456:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000269BD689A20>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000269DBE8BF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:20,456:INFO:Checking exceptions
2025-12-30 10:52:20,456:INFO:Importing libraries
2025-12-30 10:52:20,456:INFO:Copying training dataset
2025-12-30 10:52:20,458:INFO:Defining folds
2025-12-30 10:52:20,458:INFO:Declaring metric variables
2025-12-30 10:52:20,458:INFO:Importing untrained model
2025-12-30 10:52:20,459:INFO:Naive Bayes Imported successfully
2025-12-30 10:52:20,459:INFO:Starting cross validation
2025-12-30 10:52:20,460:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:20,747:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:20,753:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:20,761:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:20,761:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:20,767:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:20,769:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:20,772:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:20,773:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:20,778:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:20,782:INFO:Calculating mean and std
2025-12-30 10:52:20,782:INFO:Creating metrics dataframe
2025-12-30 10:52:20,784:INFO:Uploading results into container
2025-12-30 10:52:20,785:INFO:Uploading model into container now
2025-12-30 10:52:20,785:INFO:_master_model_container: 3
2025-12-30 10:52:20,785:INFO:_display_container: 2
2025-12-30 10:52:20,785:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-12-30 10:52:20,785:INFO:create_model() successfully completed......................................
2025-12-30 10:52:20,847:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:20,847:INFO:Creating metrics dataframe
2025-12-30 10:52:20,848:INFO:Initializing Decision Tree Classifier
2025-12-30 10:52:20,848:INFO:Total runtime is 0.20364762941996256 minutes
2025-12-30 10:52:20,848:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:20,848:INFO:Initializing create_model()
2025-12-30 10:52:20,848:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000269BD689A20>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000269DBE8BF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:20,848:INFO:Checking exceptions
2025-12-30 10:52:20,848:INFO:Importing libraries
2025-12-30 10:52:20,848:INFO:Copying training dataset
2025-12-30 10:52:20,852:INFO:Defining folds
2025-12-30 10:52:20,852:INFO:Declaring metric variables
2025-12-30 10:52:20,852:INFO:Importing untrained model
2025-12-30 10:52:20,852:INFO:Decision Tree Classifier Imported successfully
2025-12-30 10:52:20,852:INFO:Starting cross validation
2025-12-30 10:52:20,854:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:21,191:INFO:Calculating mean and std
2025-12-30 10:52:21,191:INFO:Creating metrics dataframe
2025-12-30 10:52:21,193:INFO:Uploading results into container
2025-12-30 10:52:21,193:INFO:Uploading model into container now
2025-12-30 10:52:21,193:INFO:_master_model_container: 4
2025-12-30 10:52:21,193:INFO:_display_container: 2
2025-12-30 10:52:21,194:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-12-30 10:52:21,194:INFO:create_model() successfully completed......................................
2025-12-30 10:52:21,250:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:21,250:INFO:Creating metrics dataframe
2025-12-30 10:52:21,252:INFO:Initializing SVM - Linear Kernel
2025-12-30 10:52:21,252:INFO:Total runtime is 0.21037914355595908 minutes
2025-12-30 10:52:21,252:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:21,252:INFO:Initializing create_model()
2025-12-30 10:52:21,252:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000269BD689A20>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000269DBE8BF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:21,252:INFO:Checking exceptions
2025-12-30 10:52:21,252:INFO:Importing libraries
2025-12-30 10:52:21,252:INFO:Copying training dataset
2025-12-30 10:52:21,256:INFO:Defining folds
2025-12-30 10:52:21,256:INFO:Declaring metric variables
2025-12-30 10:52:21,256:INFO:Importing untrained model
2025-12-30 10:52:21,256:INFO:SVM - Linear Kernel Imported successfully
2025-12-30 10:52:21,256:INFO:Starting cross validation
2025-12-30 10:52:21,257:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:21,516:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,518:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,523:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,524:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,530:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,531:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,549:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,555:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,562:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,565:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,572:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,573:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,573:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,573:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,575:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,575:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,578:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,579:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,579:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,599:INFO:Calculating mean and std
2025-12-30 10:52:21,600:INFO:Creating metrics dataframe
2025-12-30 10:52:21,601:INFO:Uploading results into container
2025-12-30 10:52:21,601:INFO:Uploading model into container now
2025-12-30 10:52:21,601:INFO:_master_model_container: 5
2025-12-30 10:52:21,601:INFO:_display_container: 2
2025-12-30 10:52:21,603:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-12-30 10:52:21,603:INFO:create_model() successfully completed......................................
2025-12-30 10:52:21,656:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:21,656:INFO:Creating metrics dataframe
2025-12-30 10:52:21,658:INFO:Initializing Ridge Classifier
2025-12-30 10:52:21,658:INFO:Total runtime is 0.2171394427617391 minutes
2025-12-30 10:52:21,658:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:21,658:INFO:Initializing create_model()
2025-12-30 10:52:21,658:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000269BD689A20>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000269DBE8BF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:21,658:INFO:Checking exceptions
2025-12-30 10:52:21,658:INFO:Importing libraries
2025-12-30 10:52:21,658:INFO:Copying training dataset
2025-12-30 10:52:21,660:INFO:Defining folds
2025-12-30 10:52:21,660:INFO:Declaring metric variables
2025-12-30 10:52:21,660:INFO:Importing untrained model
2025-12-30 10:52:21,660:INFO:Ridge Classifier Imported successfully
2025-12-30 10:52:21,661:INFO:Starting cross validation
2025-12-30 10:52:21,661:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:21,892:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,896:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,899:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,903:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,910:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,911:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,914:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,914:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,916:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,916:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,916:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,918:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,919:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,920:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,920:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,921:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,922:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,923:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,924:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:21,928:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:21,945:INFO:Calculating mean and std
2025-12-30 10:52:21,945:INFO:Creating metrics dataframe
2025-12-30 10:52:21,947:INFO:Uploading results into container
2025-12-30 10:52:21,947:INFO:Uploading model into container now
2025-12-30 10:52:21,947:INFO:_master_model_container: 6
2025-12-30 10:52:21,947:INFO:_display_container: 2
2025-12-30 10:52:21,948:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-12-30 10:52:21,948:INFO:create_model() successfully completed......................................
2025-12-30 10:52:22,001:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:22,001:INFO:Creating metrics dataframe
2025-12-30 10:52:22,002:INFO:Initializing Random Forest Classifier
2025-12-30 10:52:22,003:INFO:Total runtime is 0.22289018630981447 minutes
2025-12-30 10:52:22,003:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:22,003:INFO:Initializing create_model()
2025-12-30 10:52:22,003:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000269BD689A20>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000269DBE8BF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:22,003:INFO:Checking exceptions
2025-12-30 10:52:22,003:INFO:Importing libraries
2025-12-30 10:52:22,003:INFO:Copying training dataset
2025-12-30 10:52:22,005:INFO:Defining folds
2025-12-30 10:52:22,005:INFO:Declaring metric variables
2025-12-30 10:52:22,005:INFO:Importing untrained model
2025-12-30 10:52:22,005:INFO:Random Forest Classifier Imported successfully
2025-12-30 10:52:22,006:INFO:Starting cross validation
2025-12-30 10:52:22,006:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:30,596:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:52:30,596:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:52:30,596:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:52:30,596:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:52:31,386:INFO:PyCaret ClassificationExperiment
2025-12-30 10:52:31,386:INFO:Logging name: clf-default-name
2025-12-30 10:52:31,386:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-12-30 10:52:31,386:INFO:version 3.3.2
2025-12-30 10:52:31,386:INFO:Initializing setup()
2025-12-30 10:52:31,386:INFO:self.USI: 611d
2025-12-30 10:52:31,386:INFO:self._variable_keys: {'html_param', 'logging_param', 'X_train', 'fix_imbalance', 'fold_shuffle_param', 'X', 'pipeline', 'data', 'memory', 'log_plots_param', 'idx', 'y_train', 'is_multiclass', 'fold_generator', 'target_param', 'y_test', 'exp_name_log', 'gpu_param', 'gpu_n_jobs_param', 'seed', '_ml_usecase', 'USI', 'y', 'n_jobs_param', '_available_plots', 'fold_groups_param', 'exp_id', 'X_test'}
2025-12-30 10:52:31,387:INFO:Checking environment
2025-12-30 10:52:31,387:INFO:python_version: 3.10.11
2025-12-30 10:52:31,387:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-12-30 10:52:31,387:INFO:machine: AMD64
2025-12-30 10:52:31,395:INFO:platform: Windows-10-10.0.26200-SP0
2025-12-30 10:52:31,397:INFO:Memory: svmem(total=8258220032, available=2835701760, percent=65.7, used=5422518272, free=2835701760)
2025-12-30 10:52:31,397:INFO:Physical Core: 10
2025-12-30 10:52:31,397:INFO:Logical Core: 12
2025-12-30 10:52:31,397:INFO:Checking libraries
2025-12-30 10:52:31,397:INFO:System:
2025-12-30 10:52:31,397:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-12-30 10:52:31,397:INFO:executable: C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\python.exe
2025-12-30 10:52:31,397:INFO:   machine: Windows-10-10.0.26200-SP0
2025-12-30 10:52:31,397:INFO:PyCaret required dependencies:
2025-12-30 10:52:31,415:INFO:                 pip: 23.0.1
2025-12-30 10:52:31,415:INFO:          setuptools: 65.5.0
2025-12-30 10:52:31,415:INFO:             pycaret: 3.3.2
2025-12-30 10:52:31,415:INFO:             IPython: 8.37.0
2025-12-30 10:52:31,415:INFO:          ipywidgets: 8.1.8
2025-12-30 10:52:31,415:INFO:                tqdm: 4.67.1
2025-12-30 10:52:31,415:INFO:               numpy: 1.26.4
2025-12-30 10:52:31,415:INFO:              pandas: 2.1.4
2025-12-30 10:52:31,415:INFO:              jinja2: 3.1.6
2025-12-30 10:52:31,415:INFO:               scipy: 1.11.4
2025-12-30 10:52:31,415:INFO:              joblib: 1.3.2
2025-12-30 10:52:31,415:INFO:             sklearn: 1.4.2
2025-12-30 10:52:31,415:INFO:                pyod: 2.0.6
2025-12-30 10:52:31,415:INFO:            imblearn: 0.14.1
2025-12-30 10:52:31,415:INFO:   category_encoders: 2.7.0
2025-12-30 10:52:31,415:INFO:            lightgbm: 4.6.0
2025-12-30 10:52:31,415:INFO:               numba: 0.63.1
2025-12-30 10:52:31,415:INFO:            requests: 2.32.5
2025-12-30 10:52:31,415:INFO:          matplotlib: 3.7.5
2025-12-30 10:52:31,415:INFO:          scikitplot: 0.3.7
2025-12-30 10:52:31,415:INFO:         yellowbrick: 1.5
2025-12-30 10:52:31,415:INFO:              plotly: 6.5.0
2025-12-30 10:52:31,415:INFO:    plotly-resampler: Not installed
2025-12-30 10:52:31,415:INFO:             kaleido: 1.2.0
2025-12-30 10:52:31,415:INFO:           schemdraw: 0.15
2025-12-30 10:52:31,415:INFO:         statsmodels: 0.14.6
2025-12-30 10:52:31,415:INFO:              sktime: 0.26.0
2025-12-30 10:52:31,415:INFO:               tbats: 1.1.3
2025-12-30 10:52:31,415:INFO:            pmdarima: 2.0.4
2025-12-30 10:52:31,415:INFO:              psutil: 7.2.1
2025-12-30 10:52:31,415:INFO:          markupsafe: 3.0.3
2025-12-30 10:52:31,415:INFO:             pickle5: Not installed
2025-12-30 10:52:31,415:INFO:         cloudpickle: 3.1.2
2025-12-30 10:52:31,416:INFO:         deprecation: 2.1.0
2025-12-30 10:52:31,416:INFO:              xxhash: 3.6.0
2025-12-30 10:52:31,416:INFO:           wurlitzer: Not installed
2025-12-30 10:52:31,416:INFO:PyCaret optional dependencies:
2025-12-30 10:52:31,420:INFO:                shap: Not installed
2025-12-30 10:52:31,420:INFO:           interpret: Not installed
2025-12-30 10:52:31,420:INFO:                umap: Not installed
2025-12-30 10:52:31,420:INFO:     ydata_profiling: Not installed
2025-12-30 10:52:31,420:INFO:  explainerdashboard: Not installed
2025-12-30 10:52:31,420:INFO:             autoviz: Not installed
2025-12-30 10:52:31,420:INFO:           fairlearn: Not installed
2025-12-30 10:52:31,420:INFO:          deepchecks: Not installed
2025-12-30 10:52:31,420:INFO:             xgboost: Not installed
2025-12-30 10:52:31,420:INFO:            catboost: Not installed
2025-12-30 10:52:31,420:INFO:              kmodes: Not installed
2025-12-30 10:52:31,421:INFO:             mlxtend: Not installed
2025-12-30 10:52:31,421:INFO:       statsforecast: Not installed
2025-12-30 10:52:31,421:INFO:        tune_sklearn: Not installed
2025-12-30 10:52:31,421:INFO:                 ray: Not installed
2025-12-30 10:52:31,421:INFO:            hyperopt: Not installed
2025-12-30 10:52:31,421:INFO:              optuna: Not installed
2025-12-30 10:52:31,421:INFO:               skopt: Not installed
2025-12-30 10:52:31,421:INFO:              mlflow: Not installed
2025-12-30 10:52:31,421:INFO:              gradio: Not installed
2025-12-30 10:52:31,421:INFO:             fastapi: Not installed
2025-12-30 10:52:31,421:INFO:             uvicorn: Not installed
2025-12-30 10:52:31,421:INFO:              m2cgen: Not installed
2025-12-30 10:52:31,421:INFO:           evidently: Not installed
2025-12-30 10:52:31,421:INFO:               fugue: Not installed
2025-12-30 10:52:31,421:INFO:           streamlit: Not installed
2025-12-30 10:52:31,421:INFO:             prophet: Not installed
2025-12-30 10:52:31,421:INFO:None
2025-12-30 10:52:31,421:INFO:Set up data.
2025-12-30 10:52:31,425:INFO:Set up folding strategy.
2025-12-30 10:52:31,425:INFO:Set up train/test split.
2025-12-30 10:52:31,429:INFO:Set up index.
2025-12-30 10:52:31,429:INFO:Assigning column types.
2025-12-30 10:52:31,431:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-12-30 10:52:31,459:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 10:52:31,462:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:52:31,485:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,485:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,515:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 10:52:31,516:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:52:31,534:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,534:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,534:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-12-30 10:52:31,563:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:52:31,581:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,581:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,610:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:52:31,629:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,629:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,629:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-12-30 10:52:31,676:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,676:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,723:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,723:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:31,725:INFO:Preparing preprocessing pipeline...
2025-12-30 10:52:31,725:INFO:Set up simple imputation.
2025-12-30 10:52:31,728:INFO:Set up encoding of ordinal features.
2025-12-30 10:52:31,733:INFO:Set up encoding of categorical features.
2025-12-30 10:52:31,808:INFO:Finished creating preprocessing pipeline.
2025-12-30 10:52:31,869:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-12-30 10:52:31,869:INFO:Creating final display dataframe.
2025-12-30 10:52:32,053:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 25)
5   Transformed train set shape        (1674, 25)
6    Transformed test set shape         (718, 25)
7               Ignore features                 1
8              Numeric features                 3
9          Categorical features                 9
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              611d
2025-12-30 10:52:32,100:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:32,100:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:32,145:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:32,145:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:52:32,146:INFO:setup() successfully completed in 0.76s...............
2025-12-30 10:52:32,146:INFO:Initializing compare_models()
2025-12-30 10:52:32,146:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-12-30 10:52:32,146:INFO:Checking exceptions
2025-12-30 10:52:32,148:INFO:Preparing display monitor
2025-12-30 10:52:32,151:INFO:Initializing Logistic Regression
2025-12-30 10:52:32,151:INFO:Total runtime is 0.0 minutes
2025-12-30 10:52:32,151:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:32,151:INFO:Initializing create_model()
2025-12-30 10:52:32,151:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:32,151:INFO:Checking exceptions
2025-12-30 10:52:32,151:INFO:Importing libraries
2025-12-30 10:52:32,151:INFO:Copying training dataset
2025-12-30 10:52:32,155:INFO:Defining folds
2025-12-30 10:52:32,155:INFO:Declaring metric variables
2025-12-30 10:52:32,155:INFO:Importing untrained model
2025-12-30 10:52:32,155:INFO:Logistic Regression Imported successfully
2025-12-30 10:52:32,156:INFO:Starting cross validation
2025-12-30 10:52:32,157:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:40,747:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:40,749:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:40,754:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:40,782:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:40,788:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:40,808:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:40,816:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:40,829:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:40,834:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:40,883:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:40,886:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:40,890:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:40,999:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:41,001:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:41,046:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:41,053:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:41,069:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:41,093:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:52:41,108:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:41,125:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:41,143:INFO:Calculating mean and std
2025-12-30 10:52:41,145:INFO:Creating metrics dataframe
2025-12-30 10:52:41,148:INFO:Uploading results into container
2025-12-30 10:52:41,148:INFO:Uploading model into container now
2025-12-30 10:52:41,149:INFO:_master_model_container: 1
2025-12-30 10:52:41,149:INFO:_display_container: 2
2025-12-30 10:52:41,149:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 10:52:41,150:INFO:create_model() successfully completed......................................
2025-12-30 10:52:41,318:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:41,318:INFO:Creating metrics dataframe
2025-12-30 10:52:41,320:INFO:Initializing K Neighbors Classifier
2025-12-30 10:52:41,320:INFO:Total runtime is 0.15280330181121826 minutes
2025-12-30 10:52:41,320:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:41,321:INFO:Initializing create_model()
2025-12-30 10:52:41,321:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:41,321:INFO:Checking exceptions
2025-12-30 10:52:41,321:INFO:Importing libraries
2025-12-30 10:52:41,321:INFO:Copying training dataset
2025-12-30 10:52:41,326:INFO:Defining folds
2025-12-30 10:52:41,326:INFO:Declaring metric variables
2025-12-30 10:52:41,327:INFO:Importing untrained model
2025-12-30 10:52:41,327:INFO:K Neighbors Classifier Imported successfully
2025-12-30 10:52:41,327:INFO:Starting cross validation
2025-12-30 10:52:41,329:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:44,003:INFO:Calculating mean and std
2025-12-30 10:52:44,004:INFO:Creating metrics dataframe
2025-12-30 10:52:44,007:INFO:Uploading results into container
2025-12-30 10:52:44,008:INFO:Uploading model into container now
2025-12-30 10:52:44,009:INFO:_master_model_container: 2
2025-12-30 10:52:44,009:INFO:_display_container: 2
2025-12-30 10:52:44,009:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-12-30 10:52:44,009:INFO:create_model() successfully completed......................................
2025-12-30 10:52:44,101:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:44,101:INFO:Creating metrics dataframe
2025-12-30 10:52:44,103:INFO:Initializing Naive Bayes
2025-12-30 10:52:44,103:INFO:Total runtime is 0.1991903265317281 minutes
2025-12-30 10:52:44,103:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:44,104:INFO:Initializing create_model()
2025-12-30 10:52:44,104:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:44,104:INFO:Checking exceptions
2025-12-30 10:52:44,104:INFO:Importing libraries
2025-12-30 10:52:44,104:INFO:Copying training dataset
2025-12-30 10:52:44,107:INFO:Defining folds
2025-12-30 10:52:44,107:INFO:Declaring metric variables
2025-12-30 10:52:44,107:INFO:Importing untrained model
2025-12-30 10:52:44,107:INFO:Naive Bayes Imported successfully
2025-12-30 10:52:44,107:INFO:Starting cross validation
2025-12-30 10:52:44,108:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:44,374:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,376:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,376:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,381:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,382:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,384:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,387:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,389:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,390:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,400:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:44,410:INFO:Calculating mean and std
2025-12-30 10:52:44,410:INFO:Creating metrics dataframe
2025-12-30 10:52:44,412:INFO:Uploading results into container
2025-12-30 10:52:44,413:INFO:Uploading model into container now
2025-12-30 10:52:44,413:INFO:_master_model_container: 3
2025-12-30 10:52:44,413:INFO:_display_container: 2
2025-12-30 10:52:44,413:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-12-30 10:52:44,413:INFO:create_model() successfully completed......................................
2025-12-30 10:52:44,481:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:44,481:INFO:Creating metrics dataframe
2025-12-30 10:52:44,483:INFO:Initializing Decision Tree Classifier
2025-12-30 10:52:44,483:INFO:Total runtime is 0.20553269783655803 minutes
2025-12-30 10:52:44,483:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:44,485:INFO:Initializing create_model()
2025-12-30 10:52:44,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:44,485:INFO:Checking exceptions
2025-12-30 10:52:44,485:INFO:Importing libraries
2025-12-30 10:52:44,485:INFO:Copying training dataset
2025-12-30 10:52:44,488:INFO:Defining folds
2025-12-30 10:52:44,488:INFO:Declaring metric variables
2025-12-30 10:52:44,488:INFO:Importing untrained model
2025-12-30 10:52:44,488:INFO:Decision Tree Classifier Imported successfully
2025-12-30 10:52:44,488:INFO:Starting cross validation
2025-12-30 10:52:44,490:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:44,788:INFO:Calculating mean and std
2025-12-30 10:52:44,789:INFO:Creating metrics dataframe
2025-12-30 10:52:44,790:INFO:Uploading results into container
2025-12-30 10:52:44,791:INFO:Uploading model into container now
2025-12-30 10:52:44,791:INFO:_master_model_container: 4
2025-12-30 10:52:44,791:INFO:_display_container: 2
2025-12-30 10:52:44,791:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-12-30 10:52:44,791:INFO:create_model() successfully completed......................................
2025-12-30 10:52:44,849:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:44,849:INFO:Creating metrics dataframe
2025-12-30 10:52:44,852:INFO:Initializing SVM - Linear Kernel
2025-12-30 10:52:44,852:INFO:Total runtime is 0.211677889029185 minutes
2025-12-30 10:52:44,852:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:44,852:INFO:Initializing create_model()
2025-12-30 10:52:44,852:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:44,852:INFO:Checking exceptions
2025-12-30 10:52:44,852:INFO:Importing libraries
2025-12-30 10:52:44,852:INFO:Copying training dataset
2025-12-30 10:52:44,856:INFO:Defining folds
2025-12-30 10:52:44,856:INFO:Declaring metric variables
2025-12-30 10:52:44,856:INFO:Importing untrained model
2025-12-30 10:52:44,856:INFO:SVM - Linear Kernel Imported successfully
2025-12-30 10:52:44,856:INFO:Starting cross validation
2025-12-30 10:52:44,857:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:45,143:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,148:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,154:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,157:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,167:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,167:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,170:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,172:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,174:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,177:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,186:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,189:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,191:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,191:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,193:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,194:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,194:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,198:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,201:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,215:INFO:Calculating mean and std
2025-12-30 10:52:45,215:INFO:Creating metrics dataframe
2025-12-30 10:52:45,217:INFO:Uploading results into container
2025-12-30 10:52:45,217:INFO:Uploading model into container now
2025-12-30 10:52:45,218:INFO:_master_model_container: 5
2025-12-30 10:52:45,218:INFO:_display_container: 2
2025-12-30 10:52:45,218:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-12-30 10:52:45,218:INFO:create_model() successfully completed......................................
2025-12-30 10:52:45,275:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:45,275:INFO:Creating metrics dataframe
2025-12-30 10:52:45,277:INFO:Initializing Ridge Classifier
2025-12-30 10:52:45,277:INFO:Total runtime is 0.2187625805536906 minutes
2025-12-30 10:52:45,277:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:45,277:INFO:Initializing create_model()
2025-12-30 10:52:45,277:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:45,277:INFO:Checking exceptions
2025-12-30 10:52:45,277:INFO:Importing libraries
2025-12-30 10:52:45,277:INFO:Copying training dataset
2025-12-30 10:52:45,281:INFO:Defining folds
2025-12-30 10:52:45,281:INFO:Declaring metric variables
2025-12-30 10:52:45,281:INFO:Importing untrained model
2025-12-30 10:52:45,281:INFO:Ridge Classifier Imported successfully
2025-12-30 10:52:45,282:INFO:Starting cross validation
2025-12-30 10:52:45,283:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:45,532:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,539:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,539:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,543:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,543:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,545:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,545:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,548:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,549:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,550:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,552:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,556:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,557:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,558:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,559:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,562:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,564:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:45,567:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:45,575:INFO:Calculating mean and std
2025-12-30 10:52:45,575:INFO:Creating metrics dataframe
2025-12-30 10:52:45,576:INFO:Uploading results into container
2025-12-30 10:52:45,578:INFO:Uploading model into container now
2025-12-30 10:52:45,578:INFO:_master_model_container: 6
2025-12-30 10:52:45,578:INFO:_display_container: 2
2025-12-30 10:52:45,578:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-12-30 10:52:45,578:INFO:create_model() successfully completed......................................
2025-12-30 10:52:45,637:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:45,637:INFO:Creating metrics dataframe
2025-12-30 10:52:45,640:INFO:Initializing Random Forest Classifier
2025-12-30 10:52:45,640:INFO:Total runtime is 0.22480122645696005 minutes
2025-12-30 10:52:45,640:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:45,641:INFO:Initializing create_model()
2025-12-30 10:52:45,641:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:45,641:INFO:Checking exceptions
2025-12-30 10:52:45,641:INFO:Importing libraries
2025-12-30 10:52:45,641:INFO:Copying training dataset
2025-12-30 10:52:45,643:INFO:Defining folds
2025-12-30 10:52:45,643:INFO:Declaring metric variables
2025-12-30 10:52:45,643:INFO:Importing untrained model
2025-12-30 10:52:45,644:INFO:Random Forest Classifier Imported successfully
2025-12-30 10:52:45,644:INFO:Starting cross validation
2025-12-30 10:52:45,645:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:46,391:INFO:Calculating mean and std
2025-12-30 10:52:46,392:INFO:Creating metrics dataframe
2025-12-30 10:52:46,393:INFO:Uploading results into container
2025-12-30 10:52:46,393:INFO:Uploading model into container now
2025-12-30 10:52:46,394:INFO:_master_model_container: 7
2025-12-30 10:52:46,394:INFO:_display_container: 2
2025-12-30 10:52:46,394:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-12-30 10:52:46,394:INFO:create_model() successfully completed......................................
2025-12-30 10:52:46,453:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:46,453:INFO:Creating metrics dataframe
2025-12-30 10:52:46,454:INFO:Initializing Quadratic Discriminant Analysis
2025-12-30 10:52:46,454:INFO:Total runtime is 0.23838046391805012 minutes
2025-12-30 10:52:46,454:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:46,454:INFO:Initializing create_model()
2025-12-30 10:52:46,454:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:46,454:INFO:Checking exceptions
2025-12-30 10:52:46,455:INFO:Importing libraries
2025-12-30 10:52:46,455:INFO:Copying training dataset
2025-12-30 10:52:46,458:INFO:Defining folds
2025-12-30 10:52:46,458:INFO:Declaring metric variables
2025-12-30 10:52:46,458:INFO:Importing untrained model
2025-12-30 10:52:46,458:INFO:Quadratic Discriminant Analysis Imported successfully
2025-12-30 10:52:46,459:INFO:Starting cross validation
2025-12-30 10:52:46,459:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:46,628:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,641:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,644:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,646:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,656:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,659:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,664:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,664:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,665:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,691:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:52:46,706:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:46,720:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:46,720:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:46,721:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:46,726:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:46,726:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:46,731:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:46,731:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:46,749:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:46,755:INFO:Calculating mean and std
2025-12-30 10:52:46,756:INFO:Creating metrics dataframe
2025-12-30 10:52:46,758:INFO:Uploading results into container
2025-12-30 10:52:46,758:INFO:Uploading model into container now
2025-12-30 10:52:46,759:INFO:_master_model_container: 8
2025-12-30 10:52:46,759:INFO:_display_container: 2
2025-12-30 10:52:46,759:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-12-30 10:52:46,759:INFO:create_model() successfully completed......................................
2025-12-30 10:52:46,822:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:46,822:INFO:Creating metrics dataframe
2025-12-30 10:52:46,824:INFO:Initializing Ada Boost Classifier
2025-12-30 10:52:46,824:INFO:Total runtime is 0.24454040527343748 minutes
2025-12-30 10:52:46,824:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:46,824:INFO:Initializing create_model()
2025-12-30 10:52:46,825:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:46,825:INFO:Checking exceptions
2025-12-30 10:52:46,825:INFO:Importing libraries
2025-12-30 10:52:46,825:INFO:Copying training dataset
2025-12-30 10:52:46,827:INFO:Defining folds
2025-12-30 10:52:46,827:INFO:Declaring metric variables
2025-12-30 10:52:46,827:INFO:Importing untrained model
2025-12-30 10:52:46,828:INFO:Ada Boost Classifier Imported successfully
2025-12-30 10:52:46,828:INFO:Starting cross validation
2025-12-30 10:52:46,829:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:47,001:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,002:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,017:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,021:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,024:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,026:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,027:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,028:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,030:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,034:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:52:47,276:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,281:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,291:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,291:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,292:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,294:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,301:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,303:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,304:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,307:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:47,317:INFO:Calculating mean and std
2025-12-30 10:52:47,317:INFO:Creating metrics dataframe
2025-12-30 10:52:47,319:INFO:Uploading results into container
2025-12-30 10:52:47,320:INFO:Uploading model into container now
2025-12-30 10:52:47,320:INFO:_master_model_container: 9
2025-12-30 10:52:47,320:INFO:_display_container: 2
2025-12-30 10:52:47,320:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-12-30 10:52:47,320:INFO:create_model() successfully completed......................................
2025-12-30 10:52:47,378:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:47,378:INFO:Creating metrics dataframe
2025-12-30 10:52:47,380:INFO:Initializing Gradient Boosting Classifier
2025-12-30 10:52:47,380:INFO:Total runtime is 0.25380952358245845 minutes
2025-12-30 10:52:47,380:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:47,381:INFO:Initializing create_model()
2025-12-30 10:52:47,381:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:47,381:INFO:Checking exceptions
2025-12-30 10:52:47,381:INFO:Importing libraries
2025-12-30 10:52:47,381:INFO:Copying training dataset
2025-12-30 10:52:47,383:INFO:Defining folds
2025-12-30 10:52:47,383:INFO:Declaring metric variables
2025-12-30 10:52:47,383:INFO:Importing untrained model
2025-12-30 10:52:47,385:INFO:Gradient Boosting Classifier Imported successfully
2025-12-30 10:52:47,385:INFO:Starting cross validation
2025-12-30 10:52:47,386:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:49,265:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,278:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,290:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,311:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,314:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,326:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,329:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,334:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,346:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,353:INFO:Calculating mean and std
2025-12-30 10:52:49,353:INFO:Creating metrics dataframe
2025-12-30 10:52:49,355:INFO:Uploading results into container
2025-12-30 10:52:49,356:INFO:Uploading model into container now
2025-12-30 10:52:49,356:INFO:_master_model_container: 10
2025-12-30 10:52:49,356:INFO:_display_container: 2
2025-12-30 10:52:49,356:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-12-30 10:52:49,356:INFO:create_model() successfully completed......................................
2025-12-30 10:52:49,413:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:49,413:INFO:Creating metrics dataframe
2025-12-30 10:52:49,416:INFO:Initializing Linear Discriminant Analysis
2025-12-30 10:52:49,416:INFO:Total runtime is 0.28773634831110634 minutes
2025-12-30 10:52:49,416:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:49,416:INFO:Initializing create_model()
2025-12-30 10:52:49,416:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:49,416:INFO:Checking exceptions
2025-12-30 10:52:49,417:INFO:Importing libraries
2025-12-30 10:52:49,417:INFO:Copying training dataset
2025-12-30 10:52:49,419:INFO:Defining folds
2025-12-30 10:52:49,419:INFO:Declaring metric variables
2025-12-30 10:52:49,419:INFO:Importing untrained model
2025-12-30 10:52:49,419:INFO:Linear Discriminant Analysis Imported successfully
2025-12-30 10:52:49,420:INFO:Starting cross validation
2025-12-30 10:52:49,422:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:49,673:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,683:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,686:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,689:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,693:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:49,697:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,698:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,698:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,704:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,708:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:52:49,714:INFO:Calculating mean and std
2025-12-30 10:52:49,714:INFO:Creating metrics dataframe
2025-12-30 10:52:49,716:INFO:Uploading results into container
2025-12-30 10:52:49,716:INFO:Uploading model into container now
2025-12-30 10:52:49,716:INFO:_master_model_container: 11
2025-12-30 10:52:49,716:INFO:_display_container: 2
2025-12-30 10:52:49,716:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-12-30 10:52:49,717:INFO:create_model() successfully completed......................................
2025-12-30 10:52:49,773:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:49,774:INFO:Creating metrics dataframe
2025-12-30 10:52:49,777:INFO:Initializing Extra Trees Classifier
2025-12-30 10:52:49,777:INFO:Total runtime is 0.29375207026799516 minutes
2025-12-30 10:52:49,777:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:49,777:INFO:Initializing create_model()
2025-12-30 10:52:49,777:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:49,777:INFO:Checking exceptions
2025-12-30 10:52:49,777:INFO:Importing libraries
2025-12-30 10:52:49,777:INFO:Copying training dataset
2025-12-30 10:52:49,780:INFO:Defining folds
2025-12-30 10:52:49,780:INFO:Declaring metric variables
2025-12-30 10:52:49,780:INFO:Importing untrained model
2025-12-30 10:52:49,780:INFO:Extra Trees Classifier Imported successfully
2025-12-30 10:52:49,780:INFO:Starting cross validation
2025-12-30 10:52:49,781:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:50,472:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:50,512:INFO:Calculating mean and std
2025-12-30 10:52:50,513:INFO:Creating metrics dataframe
2025-12-30 10:52:50,515:INFO:Uploading results into container
2025-12-30 10:52:50,515:INFO:Uploading model into container now
2025-12-30 10:52:50,515:INFO:_master_model_container: 12
2025-12-30 10:52:50,515:INFO:_display_container: 2
2025-12-30 10:52:50,515:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-12-30 10:52:50,515:INFO:create_model() successfully completed......................................
2025-12-30 10:52:50,582:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:50,582:INFO:Creating metrics dataframe
2025-12-30 10:52:50,585:INFO:Initializing Light Gradient Boosting Machine
2025-12-30 10:52:50,585:INFO:Total runtime is 0.3072188973426818 minutes
2025-12-30 10:52:50,585:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:50,585:INFO:Initializing create_model()
2025-12-30 10:52:50,585:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:50,585:INFO:Checking exceptions
2025-12-30 10:52:50,585:INFO:Importing libraries
2025-12-30 10:52:50,586:INFO:Copying training dataset
2025-12-30 10:52:50,588:INFO:Defining folds
2025-12-30 10:52:50,588:INFO:Declaring metric variables
2025-12-30 10:52:50,588:INFO:Importing untrained model
2025-12-30 10:52:50,589:INFO:Light Gradient Boosting Machine Imported successfully
2025-12-30 10:52:50,589:INFO:Starting cross validation
2025-12-30 10:52:50,590:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:55,714:INFO:Calculating mean and std
2025-12-30 10:52:55,714:INFO:Creating metrics dataframe
2025-12-30 10:52:55,717:INFO:Uploading results into container
2025-12-30 10:52:55,717:INFO:Uploading model into container now
2025-12-30 10:52:55,717:INFO:_master_model_container: 13
2025-12-30 10:52:55,718:INFO:_display_container: 2
2025-12-30 10:52:55,718:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-12-30 10:52:55,718:INFO:create_model() successfully completed......................................
2025-12-30 10:52:55,816:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:55,817:INFO:Creating metrics dataframe
2025-12-30 10:52:55,819:INFO:Initializing Dummy Classifier
2025-12-30 10:52:55,819:INFO:Total runtime is 0.3944622596104939 minutes
2025-12-30 10:52:55,819:INFO:SubProcess create_model() called ==================================
2025-12-30 10:52:55,819:INFO:Initializing create_model()
2025-12-30 10:52:55,819:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020E0C8DBC70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:55,819:INFO:Checking exceptions
2025-12-30 10:52:55,819:INFO:Importing libraries
2025-12-30 10:52:55,819:INFO:Copying training dataset
2025-12-30 10:52:55,822:INFO:Defining folds
2025-12-30 10:52:55,823:INFO:Declaring metric variables
2025-12-30 10:52:55,823:INFO:Importing untrained model
2025-12-30 10:52:55,823:INFO:Dummy Classifier Imported successfully
2025-12-30 10:52:55,824:INFO:Starting cross validation
2025-12-30 10:52:55,826:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:52:56,162:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,174:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,192:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,198:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,199:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,199:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,202:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,203:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,207:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,207:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:52:56,215:INFO:Calculating mean and std
2025-12-30 10:52:56,216:INFO:Creating metrics dataframe
2025-12-30 10:52:56,217:INFO:Uploading results into container
2025-12-30 10:52:56,218:INFO:Uploading model into container now
2025-12-30 10:52:56,218:INFO:_master_model_container: 14
2025-12-30 10:52:56,218:INFO:_display_container: 2
2025-12-30 10:52:56,218:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-12-30 10:52:56,218:INFO:create_model() successfully completed......................................
2025-12-30 10:52:56,290:INFO:SubProcess create_model() end ==================================
2025-12-30 10:52:56,290:INFO:Creating metrics dataframe
2025-12-30 10:52:56,293:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-12-30 10:52:56,294:INFO:Initializing create_model()
2025-12-30 10:52:56,294:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020E541ABF70>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:52:56,296:INFO:Checking exceptions
2025-12-30 10:52:56,296:INFO:Importing libraries
2025-12-30 10:52:56,296:INFO:Copying training dataset
2025-12-30 10:52:56,299:INFO:Defining folds
2025-12-30 10:52:56,299:INFO:Declaring metric variables
2025-12-30 10:52:56,299:INFO:Importing untrained model
2025-12-30 10:52:56,299:INFO:Declaring custom model
2025-12-30 10:52:56,300:INFO:Gradient Boosting Classifier Imported successfully
2025-12-30 10:52:56,301:INFO:Cross validation set to False
2025-12-30 10:52:56,301:INFO:Fitting Model
2025-12-30 10:52:57,547:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-12-30 10:52:57,547:INFO:create_model() successfully completed......................................
2025-12-30 10:52:57,618:INFO:_master_model_container: 14
2025-12-30 10:52:57,618:INFO:_display_container: 2
2025-12-30 10:52:57,618:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-12-30 10:52:57,618:INFO:compare_models() successfully completed......................................
2025-12-30 10:52:57,699:INFO:Initializing save_model()
2025-12-30 10:52:57,699:INFO:save_model(model=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), model_name=student_performance_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-12-30 10:52:57,699:INFO:Adding model into prep_pipe
2025-12-30 10:52:57,716:INFO:student_performance_model.pkl saved in current working directory
2025-12-30 10:52:57,780:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Gender...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=123, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-12-30 10:52:57,780:INFO:save_model() successfully completed......................................
2025-12-30 10:56:15,489:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:56:15,490:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:56:15,490:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:56:15,490:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 10:56:16,460:INFO:PyCaret ClassificationExperiment
2025-12-30 10:56:16,461:INFO:Logging name: clf-default-name
2025-12-30 10:56:16,461:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-12-30 10:56:16,461:INFO:version 3.3.2
2025-12-30 10:56:16,461:INFO:Initializing setup()
2025-12-30 10:56:16,461:INFO:self.USI: 8b7f
2025-12-30 10:56:16,461:INFO:self._variable_keys: {'seed', 'y_test', 'USI', 'y', 'exp_id', 'idx', 'X_train', 'X', 'gpu_n_jobs_param', '_ml_usecase', 'html_param', '_available_plots', 'logging_param', 'data', 'pipeline', 'fix_imbalance', 'fold_shuffle_param', 'fold_groups_param', 'exp_name_log', 'y_train', 'X_test', 'target_param', 'is_multiclass', 'gpu_param', 'log_plots_param', 'memory', 'n_jobs_param', 'fold_generator'}
2025-12-30 10:56:16,461:INFO:Checking environment
2025-12-30 10:56:16,461:INFO:python_version: 3.10.11
2025-12-30 10:56:16,461:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-12-30 10:56:16,462:INFO:machine: AMD64
2025-12-30 10:56:16,469:INFO:platform: Windows-10-10.0.26200-SP0
2025-12-30 10:56:16,469:INFO:Memory: svmem(total=8258220032, available=1605865472, percent=80.6, used=6652354560, free=1605865472)
2025-12-30 10:56:16,470:INFO:Physical Core: 10
2025-12-30 10:56:16,470:INFO:Logical Core: 12
2025-12-30 10:56:16,470:INFO:Checking libraries
2025-12-30 10:56:16,470:INFO:System:
2025-12-30 10:56:16,470:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-12-30 10:56:16,470:INFO:executable: C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\python.exe
2025-12-30 10:56:16,470:INFO:   machine: Windows-10-10.0.26200-SP0
2025-12-30 10:56:16,470:INFO:PyCaret required dependencies:
2025-12-30 10:56:16,493:INFO:                 pip: 23.0.1
2025-12-30 10:56:16,493:INFO:          setuptools: 65.5.0
2025-12-30 10:56:16,494:INFO:             pycaret: 3.3.2
2025-12-30 10:56:16,494:INFO:             IPython: 8.37.0
2025-12-30 10:56:16,494:INFO:          ipywidgets: 8.1.8
2025-12-30 10:56:16,494:INFO:                tqdm: 4.67.1
2025-12-30 10:56:16,494:INFO:               numpy: 1.26.4
2025-12-30 10:56:16,494:INFO:              pandas: 2.1.4
2025-12-30 10:56:16,494:INFO:              jinja2: 3.1.6
2025-12-30 10:56:16,494:INFO:               scipy: 1.11.4
2025-12-30 10:56:16,494:INFO:              joblib: 1.3.2
2025-12-30 10:56:16,494:INFO:             sklearn: 1.4.2
2025-12-30 10:56:16,494:INFO:                pyod: 2.0.6
2025-12-30 10:56:16,494:INFO:            imblearn: 0.14.1
2025-12-30 10:56:16,494:INFO:   category_encoders: 2.7.0
2025-12-30 10:56:16,494:INFO:            lightgbm: 4.6.0
2025-12-30 10:56:16,494:INFO:               numba: 0.63.1
2025-12-30 10:56:16,494:INFO:            requests: 2.32.5
2025-12-30 10:56:16,494:INFO:          matplotlib: 3.7.5
2025-12-30 10:56:16,494:INFO:          scikitplot: 0.3.7
2025-12-30 10:56:16,494:INFO:         yellowbrick: 1.5
2025-12-30 10:56:16,494:INFO:              plotly: 6.5.0
2025-12-30 10:56:16,494:INFO:    plotly-resampler: Not installed
2025-12-30 10:56:16,495:INFO:             kaleido: 1.2.0
2025-12-30 10:56:16,495:INFO:           schemdraw: 0.15
2025-12-30 10:56:16,495:INFO:         statsmodels: 0.14.6
2025-12-30 10:56:16,495:INFO:              sktime: 0.26.0
2025-12-30 10:56:16,495:INFO:               tbats: 1.1.3
2025-12-30 10:56:16,495:INFO:            pmdarima: 2.0.4
2025-12-30 10:56:16,495:INFO:              psutil: 7.2.1
2025-12-30 10:56:16,495:INFO:          markupsafe: 3.0.3
2025-12-30 10:56:16,495:INFO:             pickle5: Not installed
2025-12-30 10:56:16,495:INFO:         cloudpickle: 3.1.2
2025-12-30 10:56:16,495:INFO:         deprecation: 2.1.0
2025-12-30 10:56:16,495:INFO:              xxhash: 3.6.0
2025-12-30 10:56:16,495:INFO:           wurlitzer: Not installed
2025-12-30 10:56:16,495:INFO:PyCaret optional dependencies:
2025-12-30 10:56:16,499:INFO:                shap: Not installed
2025-12-30 10:56:16,499:INFO:           interpret: Not installed
2025-12-30 10:56:16,499:INFO:                umap: Not installed
2025-12-30 10:56:16,499:INFO:     ydata_profiling: Not installed
2025-12-30 10:56:16,499:INFO:  explainerdashboard: Not installed
2025-12-30 10:56:16,499:INFO:             autoviz: Not installed
2025-12-30 10:56:16,499:INFO:           fairlearn: Not installed
2025-12-30 10:56:16,499:INFO:          deepchecks: Not installed
2025-12-30 10:56:16,499:INFO:             xgboost: Not installed
2025-12-30 10:56:16,499:INFO:            catboost: Not installed
2025-12-30 10:56:16,499:INFO:              kmodes: Not installed
2025-12-30 10:56:16,499:INFO:             mlxtend: Not installed
2025-12-30 10:56:16,499:INFO:       statsforecast: Not installed
2025-12-30 10:56:16,499:INFO:        tune_sklearn: Not installed
2025-12-30 10:56:16,499:INFO:                 ray: Not installed
2025-12-30 10:56:16,499:INFO:            hyperopt: Not installed
2025-12-30 10:56:16,499:INFO:              optuna: Not installed
2025-12-30 10:56:16,499:INFO:               skopt: Not installed
2025-12-30 10:56:16,500:INFO:              mlflow: Not installed
2025-12-30 10:56:16,500:INFO:              gradio: Not installed
2025-12-30 10:56:16,500:INFO:             fastapi: Not installed
2025-12-30 10:56:16,500:INFO:             uvicorn: Not installed
2025-12-30 10:56:16,500:INFO:              m2cgen: Not installed
2025-12-30 10:56:16,500:INFO:           evidently: Not installed
2025-12-30 10:56:16,500:INFO:               fugue: Not installed
2025-12-30 10:56:16,500:INFO:           streamlit: Not installed
2025-12-30 10:56:16,500:INFO:             prophet: Not installed
2025-12-30 10:56:16,500:INFO:None
2025-12-30 10:56:16,500:INFO:Set up data.
2025-12-30 10:56:16,508:INFO:Set up folding strategy.
2025-12-30 10:56:16,508:INFO:Set up train/test split.
2025-12-30 10:56:16,513:INFO:Set up index.
2025-12-30 10:56:16,514:INFO:Assigning column types.
2025-12-30 10:56:16,516:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-12-30 10:56:16,542:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 10:56:16,546:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:56:16,568:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,568:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,595:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 10:56:16,596:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:56:16,612:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,613:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,613:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-12-30 10:56:16,640:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:56:16,657:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,657:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,683:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 10:56:16,700:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,700:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,700:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-12-30 10:56:16,743:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,743:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,786:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,786:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:16,788:INFO:Preparing preprocessing pipeline...
2025-12-30 10:56:16,793:INFO:Set up simple imputation.
2025-12-30 10:56:16,795:INFO:Set up encoding of ordinal features.
2025-12-30 10:56:16,800:INFO:Set up encoding of categorical features.
2025-12-30 10:56:16,876:INFO:Finished creating preprocessing pipeline.
2025-12-30 10:56:16,940:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-12-30 10:56:16,940:INFO:Creating final display dataframe.
2025-12-30 10:56:17,121:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 24)
5   Transformed train set shape        (1674, 24)
6    Transformed test set shape         (718, 24)
7               Ignore features                 2
8              Numeric features                 3
9          Categorical features                 9
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              8b7f
2025-12-30 10:56:17,169:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:17,169:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:17,216:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:17,216:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 10:56:17,217:INFO:setup() successfully completed in 0.76s...............
2025-12-30 10:56:17,217:INFO:Initializing compare_models()
2025-12-30 10:56:17,217:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-12-30 10:56:17,217:INFO:Checking exceptions
2025-12-30 10:56:17,220:INFO:Preparing display monitor
2025-12-30 10:56:17,224:INFO:Initializing Logistic Regression
2025-12-30 10:56:17,224:INFO:Total runtime is 0.0 minutes
2025-12-30 10:56:17,224:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:17,224:INFO:Initializing create_model()
2025-12-30 10:56:17,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:17,225:INFO:Checking exceptions
2025-12-30 10:56:17,225:INFO:Importing libraries
2025-12-30 10:56:17,225:INFO:Copying training dataset
2025-12-30 10:56:17,228:INFO:Defining folds
2025-12-30 10:56:17,228:INFO:Declaring metric variables
2025-12-30 10:56:17,228:INFO:Importing untrained model
2025-12-30 10:56:17,228:INFO:Logistic Regression Imported successfully
2025-12-30 10:56:17,228:INFO:Starting cross validation
2025-12-30 10:56:17,229:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:24,335:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,362:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,412:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,451:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,456:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,475:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:56:24,478:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,489:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:56:24,500:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 10:56:24,515:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,579:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,581:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,589:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:24,625:INFO:Calculating mean and std
2025-12-30 10:56:24,628:INFO:Creating metrics dataframe
2025-12-30 10:56:24,633:INFO:Uploading results into container
2025-12-30 10:56:24,635:INFO:Uploading model into container now
2025-12-30 10:56:24,636:INFO:_master_model_container: 1
2025-12-30 10:56:24,637:INFO:_display_container: 2
2025-12-30 10:56:24,637:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 10:56:24,638:INFO:create_model() successfully completed......................................
2025-12-30 10:56:24,786:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:24,786:INFO:Creating metrics dataframe
2025-12-30 10:56:24,787:INFO:Initializing K Neighbors Classifier
2025-12-30 10:56:24,787:INFO:Total runtime is 0.1260552724202474 minutes
2025-12-30 10:56:24,787:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:24,788:INFO:Initializing create_model()
2025-12-30 10:56:24,788:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:24,788:INFO:Checking exceptions
2025-12-30 10:56:24,788:INFO:Importing libraries
2025-12-30 10:56:24,788:INFO:Copying training dataset
2025-12-30 10:56:24,790:INFO:Defining folds
2025-12-30 10:56:24,790:INFO:Declaring metric variables
2025-12-30 10:56:24,790:INFO:Importing untrained model
2025-12-30 10:56:24,791:INFO:K Neighbors Classifier Imported successfully
2025-12-30 10:56:24,791:INFO:Starting cross validation
2025-12-30 10:56:24,793:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:26,959:INFO:Calculating mean and std
2025-12-30 10:56:26,960:INFO:Creating metrics dataframe
2025-12-30 10:56:26,962:INFO:Uploading results into container
2025-12-30 10:56:26,962:INFO:Uploading model into container now
2025-12-30 10:56:26,963:INFO:_master_model_container: 2
2025-12-30 10:56:26,963:INFO:_display_container: 2
2025-12-30 10:56:26,963:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-12-30 10:56:26,963:INFO:create_model() successfully completed......................................
2025-12-30 10:56:27,031:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:27,031:INFO:Creating metrics dataframe
2025-12-30 10:56:27,034:INFO:Initializing Naive Bayes
2025-12-30 10:56:27,034:INFO:Total runtime is 0.16349249680836994 minutes
2025-12-30 10:56:27,034:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:27,034:INFO:Initializing create_model()
2025-12-30 10:56:27,034:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:27,034:INFO:Checking exceptions
2025-12-30 10:56:27,034:INFO:Importing libraries
2025-12-30 10:56:27,034:INFO:Copying training dataset
2025-12-30 10:56:27,036:INFO:Defining folds
2025-12-30 10:56:27,036:INFO:Declaring metric variables
2025-12-30 10:56:27,036:INFO:Importing untrained model
2025-12-30 10:56:27,037:INFO:Naive Bayes Imported successfully
2025-12-30 10:56:27,037:INFO:Starting cross validation
2025-12-30 10:56:27,038:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:27,284:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,287:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,287:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,293:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,295:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,295:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,300:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,301:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,305:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,318:INFO:Calculating mean and std
2025-12-30 10:56:27,318:INFO:Creating metrics dataframe
2025-12-30 10:56:27,319:INFO:Uploading results into container
2025-12-30 10:56:27,320:INFO:Uploading model into container now
2025-12-30 10:56:27,320:INFO:_master_model_container: 3
2025-12-30 10:56:27,320:INFO:_display_container: 2
2025-12-30 10:56:27,320:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-12-30 10:56:27,320:INFO:create_model() successfully completed......................................
2025-12-30 10:56:27,375:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:27,375:INFO:Creating metrics dataframe
2025-12-30 10:56:27,376:INFO:Initializing Decision Tree Classifier
2025-12-30 10:56:27,376:INFO:Total runtime is 0.16920021375020344 minutes
2025-12-30 10:56:27,376:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:27,376:INFO:Initializing create_model()
2025-12-30 10:56:27,376:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:27,376:INFO:Checking exceptions
2025-12-30 10:56:27,377:INFO:Importing libraries
2025-12-30 10:56:27,377:INFO:Copying training dataset
2025-12-30 10:56:27,379:INFO:Defining folds
2025-12-30 10:56:27,379:INFO:Declaring metric variables
2025-12-30 10:56:27,379:INFO:Importing untrained model
2025-12-30 10:56:27,379:INFO:Decision Tree Classifier Imported successfully
2025-12-30 10:56:27,379:INFO:Starting cross validation
2025-12-30 10:56:27,380:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:27,663:INFO:Calculating mean and std
2025-12-30 10:56:27,663:INFO:Creating metrics dataframe
2025-12-30 10:56:27,664:INFO:Uploading results into container
2025-12-30 10:56:27,665:INFO:Uploading model into container now
2025-12-30 10:56:27,665:INFO:_master_model_container: 4
2025-12-30 10:56:27,665:INFO:_display_container: 2
2025-12-30 10:56:27,665:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-12-30 10:56:27,665:INFO:create_model() successfully completed......................................
2025-12-30 10:56:27,721:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:27,721:INFO:Creating metrics dataframe
2025-12-30 10:56:27,723:INFO:Initializing SVM - Linear Kernel
2025-12-30 10:56:27,724:INFO:Total runtime is 0.17499516407648721 minutes
2025-12-30 10:56:27,724:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:27,724:INFO:Initializing create_model()
2025-12-30 10:56:27,724:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:27,724:INFO:Checking exceptions
2025-12-30 10:56:27,724:INFO:Importing libraries
2025-12-30 10:56:27,724:INFO:Copying training dataset
2025-12-30 10:56:27,726:INFO:Defining folds
2025-12-30 10:56:27,726:INFO:Declaring metric variables
2025-12-30 10:56:27,726:INFO:Importing untrained model
2025-12-30 10:56:27,726:INFO:SVM - Linear Kernel Imported successfully
2025-12-30 10:56:27,726:INFO:Starting cross validation
2025-12-30 10:56:27,727:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:27,986:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:27,989:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:27,996:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:27,998:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:27,998:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,005:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,015:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,020:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,024:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,029:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,034:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,038:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,038:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,042:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,042:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,043:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,043:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,046:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,053:INFO:Calculating mean and std
2025-12-30 10:56:28,053:INFO:Creating metrics dataframe
2025-12-30 10:56:28,055:INFO:Uploading results into container
2025-12-30 10:56:28,055:INFO:Uploading model into container now
2025-12-30 10:56:28,055:INFO:_master_model_container: 5
2025-12-30 10:56:28,055:INFO:_display_container: 2
2025-12-30 10:56:28,055:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-12-30 10:56:28,056:INFO:create_model() successfully completed......................................
2025-12-30 10:56:28,111:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:28,112:INFO:Creating metrics dataframe
2025-12-30 10:56:28,113:INFO:Initializing Ridge Classifier
2025-12-30 10:56:28,113:INFO:Total runtime is 0.18148616552352903 minutes
2025-12-30 10:56:28,113:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:28,113:INFO:Initializing create_model()
2025-12-30 10:56:28,113:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:28,113:INFO:Checking exceptions
2025-12-30 10:56:28,113:INFO:Importing libraries
2025-12-30 10:56:28,113:INFO:Copying training dataset
2025-12-30 10:56:28,117:INFO:Defining folds
2025-12-30 10:56:28,117:INFO:Declaring metric variables
2025-12-30 10:56:28,117:INFO:Importing untrained model
2025-12-30 10:56:28,117:INFO:Ridge Classifier Imported successfully
2025-12-30 10:56:28,117:INFO:Starting cross validation
2025-12-30 10:56:28,118:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:28,340:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,345:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,347:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,352:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,356:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,363:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,364:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,369:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,370:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,370:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,373:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,373:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,374:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,375:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,378:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,381:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:28,385:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:28,398:INFO:Calculating mean and std
2025-12-30 10:56:28,398:INFO:Creating metrics dataframe
2025-12-30 10:56:28,400:INFO:Uploading results into container
2025-12-30 10:56:28,400:INFO:Uploading model into container now
2025-12-30 10:56:28,400:INFO:_master_model_container: 6
2025-12-30 10:56:28,400:INFO:_display_container: 2
2025-12-30 10:56:28,400:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-12-30 10:56:28,400:INFO:create_model() successfully completed......................................
2025-12-30 10:56:28,453:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:28,454:INFO:Creating metrics dataframe
2025-12-30 10:56:28,455:INFO:Initializing Random Forest Classifier
2025-12-30 10:56:28,455:INFO:Total runtime is 0.18718236287434895 minutes
2025-12-30 10:56:28,455:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:28,455:INFO:Initializing create_model()
2025-12-30 10:56:28,455:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:28,455:INFO:Checking exceptions
2025-12-30 10:56:28,456:INFO:Importing libraries
2025-12-30 10:56:28,456:INFO:Copying training dataset
2025-12-30 10:56:28,458:INFO:Defining folds
2025-12-30 10:56:28,458:INFO:Declaring metric variables
2025-12-30 10:56:28,458:INFO:Importing untrained model
2025-12-30 10:56:28,458:INFO:Random Forest Classifier Imported successfully
2025-12-30 10:56:28,458:INFO:Starting cross validation
2025-12-30 10:56:28,459:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:29,192:INFO:Calculating mean and std
2025-12-30 10:56:29,192:INFO:Creating metrics dataframe
2025-12-30 10:56:29,193:INFO:Uploading results into container
2025-12-30 10:56:29,194:INFO:Uploading model into container now
2025-12-30 10:56:29,194:INFO:_master_model_container: 7
2025-12-30 10:56:29,194:INFO:_display_container: 2
2025-12-30 10:56:29,194:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-12-30 10:56:29,194:INFO:create_model() successfully completed......................................
2025-12-30 10:56:29,248:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:29,248:INFO:Creating metrics dataframe
2025-12-30 10:56:29,250:INFO:Initializing Quadratic Discriminant Analysis
2025-12-30 10:56:29,250:INFO:Total runtime is 0.20044012467066447 minutes
2025-12-30 10:56:29,250:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:29,250:INFO:Initializing create_model()
2025-12-30 10:56:29,250:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:29,250:INFO:Checking exceptions
2025-12-30 10:56:29,250:INFO:Importing libraries
2025-12-30 10:56:29,250:INFO:Copying training dataset
2025-12-30 10:56:29,252:INFO:Defining folds
2025-12-30 10:56:29,252:INFO:Declaring metric variables
2025-12-30 10:56:29,252:INFO:Importing untrained model
2025-12-30 10:56:29,253:INFO:Quadratic Discriminant Analysis Imported successfully
2025-12-30 10:56:29,253:INFO:Starting cross validation
2025-12-30 10:56:29,254:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:29,445:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:56:29,446:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:56:29,447:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:56:29,455:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:56:29,458:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:56:29,462:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:56:29,465:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:56:29,476:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:56:29,483:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 10:56:29,522:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,527:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,530:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,533:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,537:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,538:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,538:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,538:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,552:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,552:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:29,554:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:29,557:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:29,570:INFO:Calculating mean and std
2025-12-30 10:56:29,571:INFO:Creating metrics dataframe
2025-12-30 10:56:29,573:INFO:Uploading results into container
2025-12-30 10:56:29,573:INFO:Uploading model into container now
2025-12-30 10:56:29,574:INFO:_master_model_container: 8
2025-12-30 10:56:29,574:INFO:_display_container: 2
2025-12-30 10:56:29,574:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-12-30 10:56:29,574:INFO:create_model() successfully completed......................................
2025-12-30 10:56:29,652:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:29,653:INFO:Creating metrics dataframe
2025-12-30 10:56:29,655:INFO:Initializing Ada Boost Classifier
2025-12-30 10:56:29,655:INFO:Total runtime is 0.20717533826828002 minutes
2025-12-30 10:56:29,655:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:29,656:INFO:Initializing create_model()
2025-12-30 10:56:29,656:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:29,656:INFO:Checking exceptions
2025-12-30 10:56:29,656:INFO:Importing libraries
2025-12-30 10:56:29,656:INFO:Copying training dataset
2025-12-30 10:56:29,658:INFO:Defining folds
2025-12-30 10:56:29,658:INFO:Declaring metric variables
2025-12-30 10:56:29,658:INFO:Importing untrained model
2025-12-30 10:56:29,659:INFO:Ada Boost Classifier Imported successfully
2025-12-30 10:56:29,659:INFO:Starting cross validation
2025-12-30 10:56:29,660:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:29,853:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:29,857:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:29,868:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:29,868:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:29,886:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:29,894:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:29,896:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:29,898:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:29,912:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:29,920:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 10:56:30,184:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,189:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,190:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,210:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,213:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,228:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,254:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,263:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,265:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,295:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:30,308:INFO:Calculating mean and std
2025-12-30 10:56:30,309:INFO:Creating metrics dataframe
2025-12-30 10:56:30,311:INFO:Uploading results into container
2025-12-30 10:56:30,311:INFO:Uploading model into container now
2025-12-30 10:56:30,313:INFO:_master_model_container: 9
2025-12-30 10:56:30,313:INFO:_display_container: 2
2025-12-30 10:56:30,313:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-12-30 10:56:30,313:INFO:create_model() successfully completed......................................
2025-12-30 10:56:30,379:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:30,379:INFO:Creating metrics dataframe
2025-12-30 10:56:30,381:INFO:Initializing Gradient Boosting Classifier
2025-12-30 10:56:30,381:INFO:Total runtime is 0.219287105401357 minutes
2025-12-30 10:56:30,381:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:30,381:INFO:Initializing create_model()
2025-12-30 10:56:30,381:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:30,382:INFO:Checking exceptions
2025-12-30 10:56:30,382:INFO:Importing libraries
2025-12-30 10:56:30,382:INFO:Copying training dataset
2025-12-30 10:56:30,384:INFO:Defining folds
2025-12-30 10:56:30,384:INFO:Declaring metric variables
2025-12-30 10:56:30,384:INFO:Importing untrained model
2025-12-30 10:56:30,384:INFO:Gradient Boosting Classifier Imported successfully
2025-12-30 10:56:30,385:INFO:Starting cross validation
2025-12-30 10:56:30,386:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:32,168:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,171:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,186:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,203:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,216:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,221:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,229:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,231:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,234:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,244:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,253:INFO:Calculating mean and std
2025-12-30 10:56:32,253:INFO:Creating metrics dataframe
2025-12-30 10:56:32,255:INFO:Uploading results into container
2025-12-30 10:56:32,255:INFO:Uploading model into container now
2025-12-30 10:56:32,256:INFO:_master_model_container: 10
2025-12-30 10:56:32,256:INFO:_display_container: 2
2025-12-30 10:56:32,256:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-12-30 10:56:32,256:INFO:create_model() successfully completed......................................
2025-12-30 10:56:32,322:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:32,322:INFO:Creating metrics dataframe
2025-12-30 10:56:32,324:INFO:Initializing Linear Discriminant Analysis
2025-12-30 10:56:32,324:INFO:Total runtime is 0.2516603390375773 minutes
2025-12-30 10:56:32,324:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:32,324:INFO:Initializing create_model()
2025-12-30 10:56:32,324:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:32,324:INFO:Checking exceptions
2025-12-30 10:56:32,324:INFO:Importing libraries
2025-12-30 10:56:32,324:INFO:Copying training dataset
2025-12-30 10:56:32,327:INFO:Defining folds
2025-12-30 10:56:32,327:INFO:Declaring metric variables
2025-12-30 10:56:32,327:INFO:Importing untrained model
2025-12-30 10:56:32,327:INFO:Linear Discriminant Analysis Imported successfully
2025-12-30 10:56:32,327:INFO:Starting cross validation
2025-12-30 10:56:32,328:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:32,628:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,632:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,637:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,639:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,645:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,646:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,648:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,656:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,660:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 10:56:32,679:INFO:Calculating mean and std
2025-12-30 10:56:32,680:INFO:Creating metrics dataframe
2025-12-30 10:56:32,681:INFO:Uploading results into container
2025-12-30 10:56:32,682:INFO:Uploading model into container now
2025-12-30 10:56:32,682:INFO:_master_model_container: 11
2025-12-30 10:56:32,682:INFO:_display_container: 2
2025-12-30 10:56:32,682:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-12-30 10:56:32,682:INFO:create_model() successfully completed......................................
2025-12-30 10:56:32,754:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:32,754:INFO:Creating metrics dataframe
2025-12-30 10:56:32,756:INFO:Initializing Extra Trees Classifier
2025-12-30 10:56:32,756:INFO:Total runtime is 0.2588707089424133 minutes
2025-12-30 10:56:32,756:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:32,756:INFO:Initializing create_model()
2025-12-30 10:56:32,756:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:32,756:INFO:Checking exceptions
2025-12-30 10:56:32,756:INFO:Importing libraries
2025-12-30 10:56:32,756:INFO:Copying training dataset
2025-12-30 10:56:32,759:INFO:Defining folds
2025-12-30 10:56:32,759:INFO:Declaring metric variables
2025-12-30 10:56:32,759:INFO:Importing untrained model
2025-12-30 10:56:32,759:INFO:Extra Trees Classifier Imported successfully
2025-12-30 10:56:32,759:INFO:Starting cross validation
2025-12-30 10:56:32,760:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:33,553:INFO:Calculating mean and std
2025-12-30 10:56:33,553:INFO:Creating metrics dataframe
2025-12-30 10:56:33,555:INFO:Uploading results into container
2025-12-30 10:56:33,555:INFO:Uploading model into container now
2025-12-30 10:56:33,555:INFO:_master_model_container: 12
2025-12-30 10:56:33,555:INFO:_display_container: 2
2025-12-30 10:56:33,556:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-12-30 10:56:33,556:INFO:create_model() successfully completed......................................
2025-12-30 10:56:33,622:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:33,623:INFO:Creating metrics dataframe
2025-12-30 10:56:33,625:INFO:Initializing Light Gradient Boosting Machine
2025-12-30 10:56:33,625:INFO:Total runtime is 0.27334439357121787 minutes
2025-12-30 10:56:33,625:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:33,625:INFO:Initializing create_model()
2025-12-30 10:56:33,625:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:33,625:INFO:Checking exceptions
2025-12-30 10:56:33,625:INFO:Importing libraries
2025-12-30 10:56:33,625:INFO:Copying training dataset
2025-12-30 10:56:33,628:INFO:Defining folds
2025-12-30 10:56:33,628:INFO:Declaring metric variables
2025-12-30 10:56:33,628:INFO:Importing untrained model
2025-12-30 10:56:33,629:INFO:Light Gradient Boosting Machine Imported successfully
2025-12-30 10:56:33,629:INFO:Starting cross validation
2025-12-30 10:56:33,630:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:38,447:INFO:Calculating mean and std
2025-12-30 10:56:38,448:INFO:Creating metrics dataframe
2025-12-30 10:56:38,450:INFO:Uploading results into container
2025-12-30 10:56:38,450:INFO:Uploading model into container now
2025-12-30 10:56:38,451:INFO:_master_model_container: 13
2025-12-30 10:56:38,451:INFO:_display_container: 2
2025-12-30 10:56:38,452:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-12-30 10:56:38,452:INFO:create_model() successfully completed......................................
2025-12-30 10:56:38,524:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:38,524:INFO:Creating metrics dataframe
2025-12-30 10:56:38,526:INFO:Initializing Dummy Classifier
2025-12-30 10:56:38,527:INFO:Total runtime is 0.35504542191823324 minutes
2025-12-30 10:56:38,527:INFO:SubProcess create_model() called ==================================
2025-12-30 10:56:38,527:INFO:Initializing create_model()
2025-12-30 10:56:38,527:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75391BC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:38,527:INFO:Checking exceptions
2025-12-30 10:56:38,527:INFO:Importing libraries
2025-12-30 10:56:38,527:INFO:Copying training dataset
2025-12-30 10:56:38,530:INFO:Defining folds
2025-12-30 10:56:38,530:INFO:Declaring metric variables
2025-12-30 10:56:38,531:INFO:Importing untrained model
2025-12-30 10:56:38,531:INFO:Dummy Classifier Imported successfully
2025-12-30 10:56:38,531:INFO:Starting cross validation
2025-12-30 10:56:38,532:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 10:56:38,788:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,799:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,806:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,807:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,809:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,814:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,816:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,817:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,821:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,822:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 10:56:38,838:INFO:Calculating mean and std
2025-12-30 10:56:38,838:INFO:Creating metrics dataframe
2025-12-30 10:56:38,841:INFO:Uploading results into container
2025-12-30 10:56:38,841:INFO:Uploading model into container now
2025-12-30 10:56:38,841:INFO:_master_model_container: 14
2025-12-30 10:56:38,841:INFO:_display_container: 2
2025-12-30 10:56:38,842:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-12-30 10:56:38,842:INFO:create_model() successfully completed......................................
2025-12-30 10:56:38,901:INFO:SubProcess create_model() end ==================================
2025-12-30 10:56:38,902:INFO:Creating metrics dataframe
2025-12-30 10:56:38,906:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-12-30 10:56:38,907:INFO:Initializing create_model()
2025-12-30 10:56:38,907:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F71B44BF70>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 10:56:38,907:INFO:Checking exceptions
2025-12-30 10:56:38,908:INFO:Importing libraries
2025-12-30 10:56:38,908:INFO:Copying training dataset
2025-12-30 10:56:38,910:INFO:Defining folds
2025-12-30 10:56:38,910:INFO:Declaring metric variables
2025-12-30 10:56:38,911:INFO:Importing untrained model
2025-12-30 10:56:38,911:INFO:Declaring custom model
2025-12-30 10:56:38,911:INFO:Logistic Regression Imported successfully
2025-12-30 10:56:38,912:INFO:Cross validation set to False
2025-12-30 10:56:38,912:INFO:Fitting Model
2025-12-30 10:56:39,137:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 10:56:39,137:INFO:create_model() successfully completed......................................
2025-12-30 10:56:39,205:INFO:_master_model_container: 14
2025-12-30 10:56:39,205:INFO:_display_container: 2
2025-12-30 10:56:39,205:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 10:56:39,205:INFO:compare_models() successfully completed......................................
2025-12-30 10:56:39,264:INFO:Initializing save_model()
2025-12-30 10:56:39,264:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=student_performance_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-12-30 10:56:39,264:INFO:Adding model into prep_pipe
2025-12-30 10:56:39,272:INFO:student_performance_model.pkl saved in current working directory
2025-12-30 10:56:39,335:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Gender...
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-12-30 10:56:39,335:INFO:save_model() successfully completed......................................
2025-12-30 11:03:27,365:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 11:03:27,366:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 11:03:27,366:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 11:03:27,366:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 11:03:28,732:INFO:PyCaret ClassificationExperiment
2025-12-30 11:03:28,733:INFO:Logging name: clf-default-name
2025-12-30 11:03:28,733:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-12-30 11:03:28,733:INFO:version 3.3.2
2025-12-30 11:03:28,733:INFO:Initializing setup()
2025-12-30 11:03:28,733:INFO:self.USI: 089b
2025-12-30 11:03:28,733:INFO:self._variable_keys: {'fold_generator', 'gpu_param', '_available_plots', 'exp_name_log', 'X', 'fold_groups_param', 'X_train', 'y_train', 'is_multiclass', 'y_test', 'target_param', 'X_test', 'fix_imbalance', 'logging_param', 'pipeline', 'data', 'idx', 'log_plots_param', 'USI', 'n_jobs_param', 'gpu_n_jobs_param', 'memory', 'seed', 'y', '_ml_usecase', 'exp_id', 'html_param', 'fold_shuffle_param'}
2025-12-30 11:03:28,734:INFO:Checking environment
2025-12-30 11:03:28,734:INFO:python_version: 3.10.11
2025-12-30 11:03:28,734:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-12-30 11:03:28,734:INFO:machine: AMD64
2025-12-30 11:03:28,745:INFO:platform: Windows-10-10.0.26200-SP0
2025-12-30 11:03:28,746:INFO:Memory: svmem(total=8258220032, available=1365753856, percent=83.5, used=6892466176, free=1365753856)
2025-12-30 11:03:28,746:INFO:Physical Core: 10
2025-12-30 11:03:28,746:INFO:Logical Core: 12
2025-12-30 11:03:28,746:INFO:Checking libraries
2025-12-30 11:03:28,746:INFO:System:
2025-12-30 11:03:28,746:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-12-30 11:03:28,746:INFO:executable: C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\python.exe
2025-12-30 11:03:28,746:INFO:   machine: Windows-10-10.0.26200-SP0
2025-12-30 11:03:28,746:INFO:PyCaret required dependencies:
2025-12-30 11:03:28,778:INFO:                 pip: 23.0.1
2025-12-30 11:03:28,778:INFO:          setuptools: 65.5.0
2025-12-30 11:03:28,778:INFO:             pycaret: 3.3.2
2025-12-30 11:03:28,778:INFO:             IPython: 8.37.0
2025-12-30 11:03:28,778:INFO:          ipywidgets: 8.1.8
2025-12-30 11:03:28,778:INFO:                tqdm: 4.67.1
2025-12-30 11:03:28,778:INFO:               numpy: 1.26.4
2025-12-30 11:03:28,778:INFO:              pandas: 2.1.4
2025-12-30 11:03:28,778:INFO:              jinja2: 3.1.6
2025-12-30 11:03:28,778:INFO:               scipy: 1.11.4
2025-12-30 11:03:28,778:INFO:              joblib: 1.3.2
2025-12-30 11:03:28,778:INFO:             sklearn: 1.4.2
2025-12-30 11:03:28,778:INFO:                pyod: 2.0.6
2025-12-30 11:03:28,778:INFO:            imblearn: 0.14.1
2025-12-30 11:03:28,778:INFO:   category_encoders: 2.7.0
2025-12-30 11:03:28,778:INFO:            lightgbm: 4.6.0
2025-12-30 11:03:28,778:INFO:               numba: 0.63.1
2025-12-30 11:03:28,779:INFO:            requests: 2.32.5
2025-12-30 11:03:28,779:INFO:          matplotlib: 3.7.5
2025-12-30 11:03:28,779:INFO:          scikitplot: 0.3.7
2025-12-30 11:03:28,779:INFO:         yellowbrick: 1.5
2025-12-30 11:03:28,779:INFO:              plotly: 6.5.0
2025-12-30 11:03:28,779:INFO:    plotly-resampler: Not installed
2025-12-30 11:03:28,779:INFO:             kaleido: 1.2.0
2025-12-30 11:03:28,779:INFO:           schemdraw: 0.15
2025-12-30 11:03:28,779:INFO:         statsmodels: 0.14.6
2025-12-30 11:03:28,779:INFO:              sktime: 0.26.0
2025-12-30 11:03:28,779:INFO:               tbats: 1.1.3
2025-12-30 11:03:28,779:INFO:            pmdarima: 2.0.4
2025-12-30 11:03:28,779:INFO:              psutil: 7.2.1
2025-12-30 11:03:28,779:INFO:          markupsafe: 3.0.3
2025-12-30 11:03:28,779:INFO:             pickle5: Not installed
2025-12-30 11:03:28,779:INFO:         cloudpickle: 3.1.2
2025-12-30 11:03:28,779:INFO:         deprecation: 2.1.0
2025-12-30 11:03:28,779:INFO:              xxhash: 3.6.0
2025-12-30 11:03:28,779:INFO:           wurlitzer: Not installed
2025-12-30 11:03:28,779:INFO:PyCaret optional dependencies:
2025-12-30 11:03:28,784:INFO:                shap: Not installed
2025-12-30 11:03:28,784:INFO:           interpret: Not installed
2025-12-30 11:03:28,784:INFO:                umap: Not installed
2025-12-30 11:03:28,784:INFO:     ydata_profiling: Not installed
2025-12-30 11:03:28,784:INFO:  explainerdashboard: Not installed
2025-12-30 11:03:28,784:INFO:             autoviz: Not installed
2025-12-30 11:03:28,785:INFO:           fairlearn: Not installed
2025-12-30 11:03:28,785:INFO:          deepchecks: Not installed
2025-12-30 11:03:28,785:INFO:             xgboost: Not installed
2025-12-30 11:03:28,785:INFO:            catboost: Not installed
2025-12-30 11:03:28,785:INFO:              kmodes: Not installed
2025-12-30 11:03:28,785:INFO:             mlxtend: Not installed
2025-12-30 11:03:28,785:INFO:       statsforecast: Not installed
2025-12-30 11:03:28,785:INFO:        tune_sklearn: Not installed
2025-12-30 11:03:28,785:INFO:                 ray: Not installed
2025-12-30 11:03:28,785:INFO:            hyperopt: Not installed
2025-12-30 11:03:28,785:INFO:              optuna: Not installed
2025-12-30 11:03:28,785:INFO:               skopt: Not installed
2025-12-30 11:03:28,785:INFO:              mlflow: Not installed
2025-12-30 11:03:28,785:INFO:              gradio: Not installed
2025-12-30 11:03:28,785:INFO:             fastapi: Not installed
2025-12-30 11:03:28,785:INFO:             uvicorn: Not installed
2025-12-30 11:03:28,785:INFO:              m2cgen: Not installed
2025-12-30 11:03:28,785:INFO:           evidently: Not installed
2025-12-30 11:03:28,785:INFO:               fugue: Not installed
2025-12-30 11:03:28,785:INFO:           streamlit: Not installed
2025-12-30 11:03:28,785:INFO:             prophet: Not installed
2025-12-30 11:03:28,785:INFO:None
2025-12-30 11:03:28,785:INFO:Set up data.
2025-12-30 11:03:28,795:INFO:Set up folding strategy.
2025-12-30 11:03:28,795:INFO:Set up train/test split.
2025-12-30 11:03:28,801:INFO:Set up index.
2025-12-30 11:03:28,803:INFO:Assigning column types.
2025-12-30 11:03:28,805:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-12-30 11:03:28,835:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 11:03:28,839:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 11:03:28,914:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:28,914:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,016:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 11:03:29,019:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 11:03:29,081:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,082:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,082:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-12-30 11:03:29,186:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 11:03:29,259:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,392:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 11:03:29,475:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,476:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,478:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-12-30 11:03:29,627:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,628:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,696:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,696:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:29,702:INFO:Preparing preprocessing pipeline...
2025-12-30 11:03:29,706:INFO:Set up simple imputation.
2025-12-30 11:03:29,712:INFO:Set up encoding of ordinal features.
2025-12-30 11:03:29,720:INFO:Set up encoding of categorical features.
2025-12-30 11:03:29,898:INFO:Finished creating preprocessing pipeline.
2025-12-30 11:03:30,087:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-12-30 11:03:30,087:INFO:Creating final display dataframe.
2025-12-30 11:03:30,855:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 24)
5   Transformed train set shape        (1674, 24)
6    Transformed test set shape         (718, 24)
7               Ignore features                 2
8              Numeric features                 3
9          Categorical features                 9
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              089b
2025-12-30 11:03:31,004:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:31,005:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:31,157:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:31,158:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:03:31,158:INFO:setup() successfully completed in 2.43s...............
2025-12-30 11:03:31,158:INFO:Initializing compare_models()
2025-12-30 11:03:31,158:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-12-30 11:03:31,158:INFO:Checking exceptions
2025-12-30 11:03:31,166:INFO:Preparing display monitor
2025-12-30 11:03:31,171:INFO:Initializing Logistic Regression
2025-12-30 11:03:31,171:INFO:Total runtime is 0.0 minutes
2025-12-30 11:03:31,171:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:31,173:INFO:Initializing create_model()
2025-12-30 11:03:31,173:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:31,173:INFO:Checking exceptions
2025-12-30 11:03:31,173:INFO:Importing libraries
2025-12-30 11:03:31,173:INFO:Copying training dataset
2025-12-30 11:03:31,178:INFO:Defining folds
2025-12-30 11:03:31,180:INFO:Declaring metric variables
2025-12-30 11:03:31,180:INFO:Importing untrained model
2025-12-30 11:03:31,180:INFO:Logistic Regression Imported successfully
2025-12-30 11:03:31,180:INFO:Starting cross validation
2025-12-30 11:03:31,184:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:39,318:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:39,345:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:39,445:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:39,570:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 11:03:39,638:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:39,678:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:39,683:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 11:03:39,740:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:39,794:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:39,804:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 11:03:39,820:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:39,924:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:40,041:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:40,061:INFO:Calculating mean and std
2025-12-30 11:03:40,064:INFO:Creating metrics dataframe
2025-12-30 11:03:40,068:INFO:Uploading results into container
2025-12-30 11:03:40,069:INFO:Uploading model into container now
2025-12-30 11:03:40,070:INFO:_master_model_container: 1
2025-12-30 11:03:40,070:INFO:_display_container: 2
2025-12-30 11:03:40,071:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 11:03:40,071:INFO:create_model() successfully completed......................................
2025-12-30 11:03:40,174:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:40,174:INFO:Creating metrics dataframe
2025-12-30 11:03:40,179:INFO:Initializing K Neighbors Classifier
2025-12-30 11:03:40,179:INFO:Total runtime is 0.15011889537175496 minutes
2025-12-30 11:03:40,179:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:40,179:INFO:Initializing create_model()
2025-12-30 11:03:40,179:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:40,179:INFO:Checking exceptions
2025-12-30 11:03:40,180:INFO:Importing libraries
2025-12-30 11:03:40,180:INFO:Copying training dataset
2025-12-30 11:03:40,186:INFO:Defining folds
2025-12-30 11:03:40,186:INFO:Declaring metric variables
2025-12-30 11:03:40,186:INFO:Importing untrained model
2025-12-30 11:03:40,186:INFO:K Neighbors Classifier Imported successfully
2025-12-30 11:03:40,187:INFO:Starting cross validation
2025-12-30 11:03:40,188:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:43,745:INFO:Calculating mean and std
2025-12-30 11:03:43,746:INFO:Creating metrics dataframe
2025-12-30 11:03:43,748:INFO:Uploading results into container
2025-12-30 11:03:43,750:INFO:Uploading model into container now
2025-12-30 11:03:43,751:INFO:_master_model_container: 2
2025-12-30 11:03:43,751:INFO:_display_container: 2
2025-12-30 11:03:43,751:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-12-30 11:03:43,751:INFO:create_model() successfully completed......................................
2025-12-30 11:03:43,857:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:43,857:INFO:Creating metrics dataframe
2025-12-30 11:03:43,861:INFO:Initializing Naive Bayes
2025-12-30 11:03:43,861:INFO:Total runtime is 0.21148853699366252 minutes
2025-12-30 11:03:43,861:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:43,861:INFO:Initializing create_model()
2025-12-30 11:03:43,861:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:43,861:INFO:Checking exceptions
2025-12-30 11:03:43,862:INFO:Importing libraries
2025-12-30 11:03:43,862:INFO:Copying training dataset
2025-12-30 11:03:43,864:INFO:Defining folds
2025-12-30 11:03:43,864:INFO:Declaring metric variables
2025-12-30 11:03:43,865:INFO:Importing untrained model
2025-12-30 11:03:43,865:INFO:Naive Bayes Imported successfully
2025-12-30 11:03:43,865:INFO:Starting cross validation
2025-12-30 11:03:43,866:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:44,194:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:44,195:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:44,197:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:44,200:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:44,208:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:44,213:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:44,216:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:44,224:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:44,231:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:44,247:INFO:Calculating mean and std
2025-12-30 11:03:44,248:INFO:Creating metrics dataframe
2025-12-30 11:03:44,250:INFO:Uploading results into container
2025-12-30 11:03:44,250:INFO:Uploading model into container now
2025-12-30 11:03:44,250:INFO:_master_model_container: 3
2025-12-30 11:03:44,250:INFO:_display_container: 2
2025-12-30 11:03:44,250:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-12-30 11:03:44,251:INFO:create_model() successfully completed......................................
2025-12-30 11:03:44,310:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:44,310:INFO:Creating metrics dataframe
2025-12-30 11:03:44,311:INFO:Initializing Decision Tree Classifier
2025-12-30 11:03:44,311:INFO:Total runtime is 0.21899569431940716 minutes
2025-12-30 11:03:44,311:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:44,311:INFO:Initializing create_model()
2025-12-30 11:03:44,311:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:44,311:INFO:Checking exceptions
2025-12-30 11:03:44,313:INFO:Importing libraries
2025-12-30 11:03:44,313:INFO:Copying training dataset
2025-12-30 11:03:44,314:INFO:Defining folds
2025-12-30 11:03:44,314:INFO:Declaring metric variables
2025-12-30 11:03:44,315:INFO:Importing untrained model
2025-12-30 11:03:44,315:INFO:Decision Tree Classifier Imported successfully
2025-12-30 11:03:44,315:INFO:Starting cross validation
2025-12-30 11:03:44,316:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:44,639:INFO:Calculating mean and std
2025-12-30 11:03:44,640:INFO:Creating metrics dataframe
2025-12-30 11:03:44,641:INFO:Uploading results into container
2025-12-30 11:03:44,641:INFO:Uploading model into container now
2025-12-30 11:03:44,642:INFO:_master_model_container: 4
2025-12-30 11:03:44,642:INFO:_display_container: 2
2025-12-30 11:03:44,642:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-12-30 11:03:44,642:INFO:create_model() successfully completed......................................
2025-12-30 11:03:44,701:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:44,701:INFO:Creating metrics dataframe
2025-12-30 11:03:44,704:INFO:Initializing SVM - Linear Kernel
2025-12-30 11:03:44,704:INFO:Total runtime is 0.2255444407463074 minutes
2025-12-30 11:03:44,704:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:44,704:INFO:Initializing create_model()
2025-12-30 11:03:44,704:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:44,704:INFO:Checking exceptions
2025-12-30 11:03:44,704:INFO:Importing libraries
2025-12-30 11:03:44,704:INFO:Copying training dataset
2025-12-30 11:03:44,706:INFO:Defining folds
2025-12-30 11:03:44,706:INFO:Declaring metric variables
2025-12-30 11:03:44,706:INFO:Importing untrained model
2025-12-30 11:03:44,706:INFO:SVM - Linear Kernel Imported successfully
2025-12-30 11:03:44,706:INFO:Starting cross validation
2025-12-30 11:03:44,710:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:45,048:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,051:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,057:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,058:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,059:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,088:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,096:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,105:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,106:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,108:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,110:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,111:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,112:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,117:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,122:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,131:INFO:Calculating mean and std
2025-12-30 11:03:45,132:INFO:Creating metrics dataframe
2025-12-30 11:03:45,134:INFO:Uploading results into container
2025-12-30 11:03:45,134:INFO:Uploading model into container now
2025-12-30 11:03:45,135:INFO:_master_model_container: 5
2025-12-30 11:03:45,135:INFO:_display_container: 2
2025-12-30 11:03:45,135:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-12-30 11:03:45,135:INFO:create_model() successfully completed......................................
2025-12-30 11:03:45,196:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:45,196:INFO:Creating metrics dataframe
2025-12-30 11:03:45,199:INFO:Initializing Ridge Classifier
2025-12-30 11:03:45,199:INFO:Total runtime is 0.2337851126988729 minutes
2025-12-30 11:03:45,199:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:45,199:INFO:Initializing create_model()
2025-12-30 11:03:45,199:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:45,199:INFO:Checking exceptions
2025-12-30 11:03:45,199:INFO:Importing libraries
2025-12-30 11:03:45,199:INFO:Copying training dataset
2025-12-30 11:03:45,201:INFO:Defining folds
2025-12-30 11:03:45,201:INFO:Declaring metric variables
2025-12-30 11:03:45,201:INFO:Importing untrained model
2025-12-30 11:03:45,201:INFO:Ridge Classifier Imported successfully
2025-12-30 11:03:45,201:INFO:Starting cross validation
2025-12-30 11:03:45,203:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:45,525:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,529:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,533:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,536:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,538:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,539:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,540:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,545:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,545:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,545:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,545:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,562:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,562:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,566:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,567:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:45,569:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,569:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,574:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,575:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:45,586:INFO:Calculating mean and std
2025-12-30 11:03:45,587:INFO:Creating metrics dataframe
2025-12-30 11:03:45,591:INFO:Uploading results into container
2025-12-30 11:03:45,592:INFO:Uploading model into container now
2025-12-30 11:03:45,593:INFO:_master_model_container: 6
2025-12-30 11:03:45,593:INFO:_display_container: 2
2025-12-30 11:03:45,593:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-12-30 11:03:45,593:INFO:create_model() successfully completed......................................
2025-12-30 11:03:45,659:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:45,659:INFO:Creating metrics dataframe
2025-12-30 11:03:45,661:INFO:Initializing Random Forest Classifier
2025-12-30 11:03:45,661:INFO:Total runtime is 0.24149537483851116 minutes
2025-12-30 11:03:45,661:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:45,661:INFO:Initializing create_model()
2025-12-30 11:03:45,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:45,661:INFO:Checking exceptions
2025-12-30 11:03:45,661:INFO:Importing libraries
2025-12-30 11:03:45,661:INFO:Copying training dataset
2025-12-30 11:03:45,664:INFO:Defining folds
2025-12-30 11:03:45,664:INFO:Declaring metric variables
2025-12-30 11:03:45,664:INFO:Importing untrained model
2025-12-30 11:03:45,664:INFO:Random Forest Classifier Imported successfully
2025-12-30 11:03:45,664:INFO:Starting cross validation
2025-12-30 11:03:45,665:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:46,588:INFO:Calculating mean and std
2025-12-30 11:03:46,588:INFO:Creating metrics dataframe
2025-12-30 11:03:46,590:INFO:Uploading results into container
2025-12-30 11:03:46,591:INFO:Uploading model into container now
2025-12-30 11:03:46,591:INFO:_master_model_container: 7
2025-12-30 11:03:46,591:INFO:_display_container: 2
2025-12-30 11:03:46,591:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-12-30 11:03:46,591:INFO:create_model() successfully completed......................................
2025-12-30 11:03:46,687:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:46,688:INFO:Creating metrics dataframe
2025-12-30 11:03:46,690:INFO:Initializing Quadratic Discriminant Analysis
2025-12-30 11:03:46,690:INFO:Total runtime is 0.2586421291033427 minutes
2025-12-30 11:03:46,691:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:46,691:INFO:Initializing create_model()
2025-12-30 11:03:46,691:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:46,691:INFO:Checking exceptions
2025-12-30 11:03:46,691:INFO:Importing libraries
2025-12-30 11:03:46,691:INFO:Copying training dataset
2025-12-30 11:03:46,697:INFO:Defining folds
2025-12-30 11:03:46,697:INFO:Declaring metric variables
2025-12-30 11:03:46,697:INFO:Importing untrained model
2025-12-30 11:03:46,697:INFO:Quadratic Discriminant Analysis Imported successfully
2025-12-30 11:03:46,698:INFO:Starting cross validation
2025-12-30 11:03:46,701:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:46,947:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:03:46,947:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:03:46,948:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:03:46,953:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:03:46,960:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:03:46,961:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:03:46,971:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:03:47,022:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,022:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,022:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,024:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,026:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,026:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,029:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:47,031:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,033:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,035:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,043:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,047:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:47,059:INFO:Calculating mean and std
2025-12-30 11:03:47,059:INFO:Creating metrics dataframe
2025-12-30 11:03:47,061:INFO:Uploading results into container
2025-12-30 11:03:47,061:INFO:Uploading model into container now
2025-12-30 11:03:47,062:INFO:_master_model_container: 8
2025-12-30 11:03:47,062:INFO:_display_container: 2
2025-12-30 11:03:47,062:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-12-30 11:03:47,062:INFO:create_model() successfully completed......................................
2025-12-30 11:03:47,124:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:47,124:INFO:Creating metrics dataframe
2025-12-30 11:03:47,125:INFO:Initializing Ada Boost Classifier
2025-12-30 11:03:47,126:INFO:Total runtime is 0.26589392423629765 minutes
2025-12-30 11:03:47,126:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:47,126:INFO:Initializing create_model()
2025-12-30 11:03:47,126:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:47,126:INFO:Checking exceptions
2025-12-30 11:03:47,126:INFO:Importing libraries
2025-12-30 11:03:47,126:INFO:Copying training dataset
2025-12-30 11:03:47,129:INFO:Defining folds
2025-12-30 11:03:47,129:INFO:Declaring metric variables
2025-12-30 11:03:47,129:INFO:Importing untrained model
2025-12-30 11:03:47,130:INFO:Ada Boost Classifier Imported successfully
2025-12-30 11:03:47,130:INFO:Starting cross validation
2025-12-30 11:03:47,131:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:47,318:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,320:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,328:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,332:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,334:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,337:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,341:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,353:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,356:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,358:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:03:47,656:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,663:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,664:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,672:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,682:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,684:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,685:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,694:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,696:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,701:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:47,723:INFO:Calculating mean and std
2025-12-30 11:03:47,725:INFO:Creating metrics dataframe
2025-12-30 11:03:47,727:INFO:Uploading results into container
2025-12-30 11:03:47,727:INFO:Uploading model into container now
2025-12-30 11:03:47,727:INFO:_master_model_container: 9
2025-12-30 11:03:47,727:INFO:_display_container: 2
2025-12-30 11:03:47,728:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-12-30 11:03:47,728:INFO:create_model() successfully completed......................................
2025-12-30 11:03:47,790:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:47,790:INFO:Creating metrics dataframe
2025-12-30 11:03:47,792:INFO:Initializing Gradient Boosting Classifier
2025-12-30 11:03:47,792:INFO:Total runtime is 0.2770161628723145 minutes
2025-12-30 11:03:47,792:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:47,792:INFO:Initializing create_model()
2025-12-30 11:03:47,792:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:47,792:INFO:Checking exceptions
2025-12-30 11:03:47,792:INFO:Importing libraries
2025-12-30 11:03:47,792:INFO:Copying training dataset
2025-12-30 11:03:47,795:INFO:Defining folds
2025-12-30 11:03:47,795:INFO:Declaring metric variables
2025-12-30 11:03:47,795:INFO:Importing untrained model
2025-12-30 11:03:47,795:INFO:Gradient Boosting Classifier Imported successfully
2025-12-30 11:03:47,796:INFO:Starting cross validation
2025-12-30 11:03:47,796:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:49,830:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,849:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,863:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,878:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,886:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,894:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,903:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,905:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,906:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,914:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:49,934:INFO:Calculating mean and std
2025-12-30 11:03:49,935:INFO:Creating metrics dataframe
2025-12-30 11:03:49,938:INFO:Uploading results into container
2025-12-30 11:03:49,939:INFO:Uploading model into container now
2025-12-30 11:03:49,940:INFO:_master_model_container: 10
2025-12-30 11:03:49,940:INFO:_display_container: 2
2025-12-30 11:03:49,941:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-12-30 11:03:49,941:INFO:create_model() successfully completed......................................
2025-12-30 11:03:50,020:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:50,020:INFO:Creating metrics dataframe
2025-12-30 11:03:50,023:INFO:Initializing Linear Discriminant Analysis
2025-12-30 11:03:50,023:INFO:Total runtime is 0.3141845981280009 minutes
2025-12-30 11:03:50,023:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:50,023:INFO:Initializing create_model()
2025-12-30 11:03:50,023:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:50,023:INFO:Checking exceptions
2025-12-30 11:03:50,023:INFO:Importing libraries
2025-12-30 11:03:50,023:INFO:Copying training dataset
2025-12-30 11:03:50,025:INFO:Defining folds
2025-12-30 11:03:50,025:INFO:Declaring metric variables
2025-12-30 11:03:50,026:INFO:Importing untrained model
2025-12-30 11:03:50,026:INFO:Linear Discriminant Analysis Imported successfully
2025-12-30 11:03:50,026:INFO:Starting cross validation
2025-12-30 11:03:50,027:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:50,318:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:50,322:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:50,329:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:50,335:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:50,340:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:50,340:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:50,341:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:50,351:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:50,354:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:03:50,377:INFO:Calculating mean and std
2025-12-30 11:03:50,378:INFO:Creating metrics dataframe
2025-12-30 11:03:50,381:INFO:Uploading results into container
2025-12-30 11:03:50,382:INFO:Uploading model into container now
2025-12-30 11:03:50,382:INFO:_master_model_container: 11
2025-12-30 11:03:50,382:INFO:_display_container: 2
2025-12-30 11:03:50,383:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-12-30 11:03:50,383:INFO:create_model() successfully completed......................................
2025-12-30 11:03:50,447:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:50,447:INFO:Creating metrics dataframe
2025-12-30 11:03:50,451:INFO:Initializing Extra Trees Classifier
2025-12-30 11:03:50,451:INFO:Total runtime is 0.32132097482681277 minutes
2025-12-30 11:03:50,451:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:50,451:INFO:Initializing create_model()
2025-12-30 11:03:50,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:50,451:INFO:Checking exceptions
2025-12-30 11:03:50,451:INFO:Importing libraries
2025-12-30 11:03:50,451:INFO:Copying training dataset
2025-12-30 11:03:50,453:INFO:Defining folds
2025-12-30 11:03:50,453:INFO:Declaring metric variables
2025-12-30 11:03:50,453:INFO:Importing untrained model
2025-12-30 11:03:50,453:INFO:Extra Trees Classifier Imported successfully
2025-12-30 11:03:50,454:INFO:Starting cross validation
2025-12-30 11:03:50,454:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:51,469:INFO:Calculating mean and std
2025-12-30 11:03:51,470:INFO:Creating metrics dataframe
2025-12-30 11:03:51,472:INFO:Uploading results into container
2025-12-30 11:03:51,473:INFO:Uploading model into container now
2025-12-30 11:03:51,473:INFO:_master_model_container: 12
2025-12-30 11:03:51,473:INFO:_display_container: 2
2025-12-30 11:03:51,474:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-12-30 11:03:51,474:INFO:create_model() successfully completed......................................
2025-12-30 11:03:51,544:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:51,544:INFO:Creating metrics dataframe
2025-12-30 11:03:51,545:INFO:Initializing Light Gradient Boosting Machine
2025-12-30 11:03:51,545:INFO:Total runtime is 0.3395528395970663 minutes
2025-12-30 11:03:51,546:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:51,546:INFO:Initializing create_model()
2025-12-30 11:03:51,546:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:51,546:INFO:Checking exceptions
2025-12-30 11:03:51,546:INFO:Importing libraries
2025-12-30 11:03:51,546:INFO:Copying training dataset
2025-12-30 11:03:51,547:INFO:Defining folds
2025-12-30 11:03:51,547:INFO:Declaring metric variables
2025-12-30 11:03:51,547:INFO:Importing untrained model
2025-12-30 11:03:51,549:INFO:Light Gradient Boosting Machine Imported successfully
2025-12-30 11:03:51,549:INFO:Starting cross validation
2025-12-30 11:03:51,550:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:57,518:INFO:Calculating mean and std
2025-12-30 11:03:57,518:INFO:Creating metrics dataframe
2025-12-30 11:03:57,523:INFO:Uploading results into container
2025-12-30 11:03:57,524:INFO:Uploading model into container now
2025-12-30 11:03:57,524:INFO:_master_model_container: 13
2025-12-30 11:03:57,524:INFO:_display_container: 2
2025-12-30 11:03:57,525:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-12-30 11:03:57,525:INFO:create_model() successfully completed......................................
2025-12-30 11:03:57,616:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:57,616:INFO:Creating metrics dataframe
2025-12-30 11:03:57,620:INFO:Initializing Dummy Classifier
2025-12-30 11:03:57,620:INFO:Total runtime is 0.4408167084058126 minutes
2025-12-30 11:03:57,620:INFO:SubProcess create_model() called ==================================
2025-12-30 11:03:57,620:INFO:Initializing create_model()
2025-12-30 11:03:57,620:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021C8AD8B9A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:57,620:INFO:Checking exceptions
2025-12-30 11:03:57,620:INFO:Importing libraries
2025-12-30 11:03:57,622:INFO:Copying training dataset
2025-12-30 11:03:57,624:INFO:Defining folds
2025-12-30 11:03:57,624:INFO:Declaring metric variables
2025-12-30 11:03:57,624:INFO:Importing untrained model
2025-12-30 11:03:57,625:INFO:Dummy Classifier Imported successfully
2025-12-30 11:03:57,625:INFO:Starting cross validation
2025-12-30 11:03:57,626:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:03:58,035:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,067:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,072:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,076:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,082:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,098:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,113:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,125:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,127:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,132:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:03:58,143:INFO:Calculating mean and std
2025-12-30 11:03:58,144:INFO:Creating metrics dataframe
2025-12-30 11:03:58,146:INFO:Uploading results into container
2025-12-30 11:03:58,146:INFO:Uploading model into container now
2025-12-30 11:03:58,146:INFO:_master_model_container: 14
2025-12-30 11:03:58,146:INFO:_display_container: 2
2025-12-30 11:03:58,146:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-12-30 11:03:58,146:INFO:create_model() successfully completed......................................
2025-12-30 11:03:58,217:INFO:SubProcess create_model() end ==================================
2025-12-30 11:03:58,217:INFO:Creating metrics dataframe
2025-12-30 11:03:58,224:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-12-30 11:03:58,226:INFO:Initializing create_model()
2025-12-30 11:03:58,226:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CD26FBF70>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:03:58,226:INFO:Checking exceptions
2025-12-30 11:03:58,226:INFO:Importing libraries
2025-12-30 11:03:58,226:INFO:Copying training dataset
2025-12-30 11:03:58,228:INFO:Defining folds
2025-12-30 11:03:58,230:INFO:Declaring metric variables
2025-12-30 11:03:58,230:INFO:Importing untrained model
2025-12-30 11:03:58,230:INFO:Declaring custom model
2025-12-30 11:03:58,230:INFO:Logistic Regression Imported successfully
2025-12-30 11:03:58,231:INFO:Cross validation set to False
2025-12-30 11:03:58,231:INFO:Fitting Model
2025-12-30 11:03:58,618:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 11:03:58,619:INFO:create_model() successfully completed......................................
2025-12-30 11:03:58,700:INFO:_master_model_container: 14
2025-12-30 11:03:58,700:INFO:_display_container: 2
2025-12-30 11:03:58,700:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 11:03:58,701:INFO:compare_models() successfully completed......................................
2025-12-30 11:03:58,831:INFO:Initializing save_model()
2025-12-30 11:03:58,831:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=student_performance_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-12-30 11:03:58,831:INFO:Adding model into prep_pipe
2025-12-30 11:03:58,845:INFO:student_performance_model.pkl saved in current working directory
2025-12-30 11:03:59,006:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Gender...
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-12-30 11:03:59,006:INFO:save_model() successfully completed......................................
2025-12-30 11:03:59,090:ERROR:
'fastapi' is a soft dependency and not included in the pycaret installation. Please run: `pip install fastapi` to install.
Alternately, you can install this by running `pip install pycaret[mlops]`
NoneType: None
2025-12-30 11:06:25,626:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 11:06:25,627:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 11:06:25,627:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 11:06:25,627:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 11:06:26,877:INFO:PyCaret ClassificationExperiment
2025-12-30 11:06:26,878:INFO:Logging name: clf-default-name
2025-12-30 11:06:26,878:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-12-30 11:06:26,878:INFO:version 3.3.2
2025-12-30 11:06:26,878:INFO:Initializing setup()
2025-12-30 11:06:26,878:INFO:self.USI: 009d
2025-12-30 11:06:26,878:INFO:self._variable_keys: {'logging_param', 'X_train', 'y', 'exp_id', 'exp_name_log', 'is_multiclass', 'n_jobs_param', 'gpu_n_jobs_param', '_ml_usecase', 'idx', 'memory', 'X', 'fold_shuffle_param', 'y_train', 'X_test', 'pipeline', 'target_param', 'seed', 'fix_imbalance', '_available_plots', 'gpu_param', 'fold_generator', 'html_param', 'fold_groups_param', 'data', 'USI', 'y_test', 'log_plots_param'}
2025-12-30 11:06:26,878:INFO:Checking environment
2025-12-30 11:06:26,878:INFO:python_version: 3.10.11
2025-12-30 11:06:26,878:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-12-30 11:06:26,879:INFO:machine: AMD64
2025-12-30 11:06:26,889:INFO:platform: Windows-10-10.0.26200-SP0
2025-12-30 11:06:26,889:INFO:Memory: svmem(total=8258220032, available=992579584, percent=88.0, used=7265640448, free=992579584)
2025-12-30 11:06:26,889:INFO:Physical Core: 10
2025-12-30 11:06:26,889:INFO:Logical Core: 12
2025-12-30 11:06:26,889:INFO:Checking libraries
2025-12-30 11:06:26,890:INFO:System:
2025-12-30 11:06:26,890:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-12-30 11:06:26,890:INFO:executable: C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\python.exe
2025-12-30 11:06:26,890:INFO:   machine: Windows-10-10.0.26200-SP0
2025-12-30 11:06:26,890:INFO:PyCaret required dependencies:
2025-12-30 11:06:26,915:INFO:                 pip: 23.0.1
2025-12-30 11:06:26,915:INFO:          setuptools: 65.5.0
2025-12-30 11:06:26,915:INFO:             pycaret: 3.3.2
2025-12-30 11:06:26,915:INFO:             IPython: 8.37.0
2025-12-30 11:06:26,916:INFO:          ipywidgets: 8.1.8
2025-12-30 11:06:26,916:INFO:                tqdm: 4.67.1
2025-12-30 11:06:26,916:INFO:               numpy: 1.26.4
2025-12-30 11:06:26,916:INFO:              pandas: 2.1.4
2025-12-30 11:06:26,916:INFO:              jinja2: 3.1.6
2025-12-30 11:06:26,916:INFO:               scipy: 1.11.4
2025-12-30 11:06:26,916:INFO:              joblib: 1.3.2
2025-12-30 11:06:26,916:INFO:             sklearn: 1.4.2
2025-12-30 11:06:26,916:INFO:                pyod: 2.0.6
2025-12-30 11:06:26,916:INFO:            imblearn: 0.14.1
2025-12-30 11:06:26,916:INFO:   category_encoders: 2.7.0
2025-12-30 11:06:26,916:INFO:            lightgbm: 4.6.0
2025-12-30 11:06:26,916:INFO:               numba: 0.63.1
2025-12-30 11:06:26,916:INFO:            requests: 2.32.5
2025-12-30 11:06:26,916:INFO:          matplotlib: 3.7.5
2025-12-30 11:06:26,917:INFO:          scikitplot: 0.3.7
2025-12-30 11:06:26,917:INFO:         yellowbrick: 1.5
2025-12-30 11:06:26,917:INFO:              plotly: 6.5.0
2025-12-30 11:06:26,917:INFO:    plotly-resampler: Not installed
2025-12-30 11:06:26,917:INFO:             kaleido: 1.2.0
2025-12-30 11:06:26,917:INFO:           schemdraw: 0.15
2025-12-30 11:06:26,917:INFO:         statsmodels: 0.14.6
2025-12-30 11:06:26,917:INFO:              sktime: 0.26.0
2025-12-30 11:06:26,917:INFO:               tbats: 1.1.3
2025-12-30 11:06:26,917:INFO:            pmdarima: 2.0.4
2025-12-30 11:06:26,917:INFO:              psutil: 7.2.1
2025-12-30 11:06:26,917:INFO:          markupsafe: 3.0.3
2025-12-30 11:06:26,917:INFO:             pickle5: Not installed
2025-12-30 11:06:26,917:INFO:         cloudpickle: 3.1.2
2025-12-30 11:06:26,917:INFO:         deprecation: 2.1.0
2025-12-30 11:06:26,917:INFO:              xxhash: 3.6.0
2025-12-30 11:06:26,917:INFO:           wurlitzer: Not installed
2025-12-30 11:06:26,917:INFO:PyCaret optional dependencies:
2025-12-30 11:06:27,653:INFO:                shap: Not installed
2025-12-30 11:06:27,653:INFO:           interpret: Not installed
2025-12-30 11:06:27,653:INFO:                umap: Not installed
2025-12-30 11:06:27,653:INFO:     ydata_profiling: Not installed
2025-12-30 11:06:27,653:INFO:  explainerdashboard: Not installed
2025-12-30 11:06:27,653:INFO:             autoviz: Not installed
2025-12-30 11:06:27,653:INFO:           fairlearn: Not installed
2025-12-30 11:06:27,653:INFO:          deepchecks: Not installed
2025-12-30 11:06:27,653:INFO:             xgboost: Not installed
2025-12-30 11:06:27,653:INFO:            catboost: Not installed
2025-12-30 11:06:27,653:INFO:              kmodes: Not installed
2025-12-30 11:06:27,653:INFO:             mlxtend: Not installed
2025-12-30 11:06:27,653:INFO:       statsforecast: Not installed
2025-12-30 11:06:27,653:INFO:        tune_sklearn: Not installed
2025-12-30 11:06:27,653:INFO:                 ray: Not installed
2025-12-30 11:06:27,653:INFO:            hyperopt: Not installed
2025-12-30 11:06:27,653:INFO:              optuna: Not installed
2025-12-30 11:06:27,653:INFO:               skopt: Not installed
2025-12-30 11:06:27,653:INFO:              mlflow: Not installed
2025-12-30 11:06:27,653:INFO:              gradio: Not installed
2025-12-30 11:06:27,653:INFO:             fastapi: 0.128.0
2025-12-30 11:06:27,653:INFO:             uvicorn: Not installed
2025-12-30 11:06:27,653:INFO:              m2cgen: Not installed
2025-12-30 11:06:27,653:INFO:           evidently: Not installed
2025-12-30 11:06:27,653:INFO:               fugue: Not installed
2025-12-30 11:06:27,654:INFO:           streamlit: Not installed
2025-12-30 11:06:27,654:INFO:             prophet: Not installed
2025-12-30 11:06:27,654:INFO:None
2025-12-30 11:06:27,654:INFO:Set up data.
2025-12-30 11:06:27,659:INFO:Set up folding strategy.
2025-12-30 11:06:27,659:INFO:Set up train/test split.
2025-12-30 11:06:27,663:INFO:Set up index.
2025-12-30 11:06:27,663:INFO:Assigning column types.
2025-12-30 11:06:27,665:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-12-30 11:06:27,699:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 11:06:27,702:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 11:06:27,731:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:27,731:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:27,766:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-12-30 11:06:27,767:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 11:06:27,788:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:27,788:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:27,788:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-12-30 11:06:27,823:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 11:06:27,844:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:27,844:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:27,884:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-12-30 11:06:27,906:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:27,906:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:27,906:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-12-30 11:06:27,969:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:27,969:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:28,038:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:28,038:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:28,040:INFO:Preparing preprocessing pipeline...
2025-12-30 11:06:28,042:INFO:Set up simple imputation.
2025-12-30 11:06:28,045:INFO:Set up encoding of ordinal features.
2025-12-30 11:06:28,051:INFO:Set up encoding of categorical features.
2025-12-30 11:06:28,166:INFO:Finished creating preprocessing pipeline.
2025-12-30 11:06:28,263:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-12-30 11:06:28,263:INFO:Creating final display dataframe.
2025-12-30 11:06:28,562:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 24)
5   Transformed train set shape        (1674, 24)
6    Transformed test set shape         (718, 24)
7               Ignore features                 2
8              Numeric features                 3
9          Categorical features                 9
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              009d
2025-12-30 11:06:28,644:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:28,644:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:28,725:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:28,726:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-12-30 11:06:28,727:INFO:setup() successfully completed in 1.85s...............
2025-12-30 11:06:28,728:INFO:Initializing compare_models()
2025-12-30 11:06:28,728:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-12-30 11:06:28,728:INFO:Checking exceptions
2025-12-30 11:06:28,733:INFO:Preparing display monitor
2025-12-30 11:06:28,737:INFO:Initializing Logistic Regression
2025-12-30 11:06:28,737:INFO:Total runtime is 0.0 minutes
2025-12-30 11:06:28,738:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:28,738:INFO:Initializing create_model()
2025-12-30 11:06:28,738:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:28,738:INFO:Checking exceptions
2025-12-30 11:06:28,738:INFO:Importing libraries
2025-12-30 11:06:28,738:INFO:Copying training dataset
2025-12-30 11:06:28,745:INFO:Defining folds
2025-12-30 11:06:28,746:INFO:Declaring metric variables
2025-12-30 11:06:28,746:INFO:Importing untrained model
2025-12-30 11:06:28,746:INFO:Logistic Regression Imported successfully
2025-12-30 11:06:28,746:INFO:Starting cross validation
2025-12-30 11:06:28,749:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:38,827:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:38,842:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:38,849:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:38,899:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:38,921:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:38,922:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 11:06:38,987:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:38,990:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:39,023:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:39,031:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 11:06:39,135:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:39,238:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-12-30 11:06:39,277:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:39,301:INFO:Calculating mean and std
2025-12-30 11:06:39,302:INFO:Creating metrics dataframe
2025-12-30 11:06:39,305:INFO:Uploading results into container
2025-12-30 11:06:39,305:INFO:Uploading model into container now
2025-12-30 11:06:39,306:INFO:_master_model_container: 1
2025-12-30 11:06:39,306:INFO:_display_container: 2
2025-12-30 11:06:39,307:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 11:06:39,307:INFO:create_model() successfully completed......................................
2025-12-30 11:06:39,449:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:39,450:INFO:Creating metrics dataframe
2025-12-30 11:06:39,453:INFO:Initializing K Neighbors Classifier
2025-12-30 11:06:39,453:INFO:Total runtime is 0.17860979239145916 minutes
2025-12-30 11:06:39,453:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:39,453:INFO:Initializing create_model()
2025-12-30 11:06:39,453:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:39,453:INFO:Checking exceptions
2025-12-30 11:06:39,453:INFO:Importing libraries
2025-12-30 11:06:39,453:INFO:Copying training dataset
2025-12-30 11:06:39,456:INFO:Defining folds
2025-12-30 11:06:39,456:INFO:Declaring metric variables
2025-12-30 11:06:39,456:INFO:Importing untrained model
2025-12-30 11:06:39,456:INFO:K Neighbors Classifier Imported successfully
2025-12-30 11:06:39,458:INFO:Starting cross validation
2025-12-30 11:06:39,458:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:42,096:INFO:Calculating mean and std
2025-12-30 11:06:42,098:INFO:Creating metrics dataframe
2025-12-30 11:06:42,101:INFO:Uploading results into container
2025-12-30 11:06:42,101:INFO:Uploading model into container now
2025-12-30 11:06:42,102:INFO:_master_model_container: 2
2025-12-30 11:06:42,102:INFO:_display_container: 2
2025-12-30 11:06:42,103:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-12-30 11:06:42,103:INFO:create_model() successfully completed......................................
2025-12-30 11:06:42,193:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:42,193:INFO:Creating metrics dataframe
2025-12-30 11:06:42,195:INFO:Initializing Naive Bayes
2025-12-30 11:06:42,195:INFO:Total runtime is 0.22431352535883586 minutes
2025-12-30 11:06:42,195:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:42,195:INFO:Initializing create_model()
2025-12-30 11:06:42,195:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:42,196:INFO:Checking exceptions
2025-12-30 11:06:42,196:INFO:Importing libraries
2025-12-30 11:06:42,196:INFO:Copying training dataset
2025-12-30 11:06:42,199:INFO:Defining folds
2025-12-30 11:06:42,199:INFO:Declaring metric variables
2025-12-30 11:06:42,199:INFO:Importing untrained model
2025-12-30 11:06:42,199:INFO:Naive Bayes Imported successfully
2025-12-30 11:06:42,199:INFO:Starting cross validation
2025-12-30 11:06:42,200:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:42,493:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,512:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,516:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,517:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,517:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,522:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,526:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,526:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,530:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,531:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:42,551:INFO:Calculating mean and std
2025-12-30 11:06:42,551:INFO:Creating metrics dataframe
2025-12-30 11:06:42,552:INFO:Uploading results into container
2025-12-30 11:06:42,553:INFO:Uploading model into container now
2025-12-30 11:06:42,553:INFO:_master_model_container: 3
2025-12-30 11:06:42,553:INFO:_display_container: 2
2025-12-30 11:06:42,553:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-12-30 11:06:42,553:INFO:create_model() successfully completed......................................
2025-12-30 11:06:42,628:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:42,629:INFO:Creating metrics dataframe
2025-12-30 11:06:42,631:INFO:Initializing Decision Tree Classifier
2025-12-30 11:06:42,631:INFO:Total runtime is 0.23157828251520793 minutes
2025-12-30 11:06:42,631:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:42,631:INFO:Initializing create_model()
2025-12-30 11:06:42,631:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:42,631:INFO:Checking exceptions
2025-12-30 11:06:42,631:INFO:Importing libraries
2025-12-30 11:06:42,631:INFO:Copying training dataset
2025-12-30 11:06:42,635:INFO:Defining folds
2025-12-30 11:06:42,635:INFO:Declaring metric variables
2025-12-30 11:06:42,635:INFO:Importing untrained model
2025-12-30 11:06:42,635:INFO:Decision Tree Classifier Imported successfully
2025-12-30 11:06:42,635:INFO:Starting cross validation
2025-12-30 11:06:42,636:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:42,960:INFO:Calculating mean and std
2025-12-30 11:06:42,961:INFO:Creating metrics dataframe
2025-12-30 11:06:42,962:INFO:Uploading results into container
2025-12-30 11:06:42,962:INFO:Uploading model into container now
2025-12-30 11:06:42,963:INFO:_master_model_container: 4
2025-12-30 11:06:42,963:INFO:_display_container: 2
2025-12-30 11:06:42,963:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-12-30 11:06:42,963:INFO:create_model() successfully completed......................................
2025-12-30 11:06:43,030:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:43,030:INFO:Creating metrics dataframe
2025-12-30 11:06:43,031:INFO:Initializing SVM - Linear Kernel
2025-12-30 11:06:43,031:INFO:Total runtime is 0.23824780384699504 minutes
2025-12-30 11:06:43,031:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:43,033:INFO:Initializing create_model()
2025-12-30 11:06:43,033:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:43,033:INFO:Checking exceptions
2025-12-30 11:06:43,033:INFO:Importing libraries
2025-12-30 11:06:43,033:INFO:Copying training dataset
2025-12-30 11:06:43,035:INFO:Defining folds
2025-12-30 11:06:43,035:INFO:Declaring metric variables
2025-12-30 11:06:43,035:INFO:Importing untrained model
2025-12-30 11:06:43,035:INFO:SVM - Linear Kernel Imported successfully
2025-12-30 11:06:43,035:INFO:Starting cross validation
2025-12-30 11:06:43,036:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:43,337:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,342:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,342:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,346:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,349:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,380:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,384:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,385:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,386:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,390:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,391:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,395:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,396:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,399:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,399:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,401:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,405:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,416:INFO:Calculating mean and std
2025-12-30 11:06:43,416:INFO:Creating metrics dataframe
2025-12-30 11:06:43,418:INFO:Uploading results into container
2025-12-30 11:06:43,418:INFO:Uploading model into container now
2025-12-30 11:06:43,418:INFO:_master_model_container: 5
2025-12-30 11:06:43,418:INFO:_display_container: 2
2025-12-30 11:06:43,419:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-12-30 11:06:43,419:INFO:create_model() successfully completed......................................
2025-12-30 11:06:43,483:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:43,483:INFO:Creating metrics dataframe
2025-12-30 11:06:43,485:INFO:Initializing Ridge Classifier
2025-12-30 11:06:43,485:INFO:Total runtime is 0.24581462542215984 minutes
2025-12-30 11:06:43,485:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:43,485:INFO:Initializing create_model()
2025-12-30 11:06:43,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:43,485:INFO:Checking exceptions
2025-12-30 11:06:43,485:INFO:Importing libraries
2025-12-30 11:06:43,485:INFO:Copying training dataset
2025-12-30 11:06:43,488:INFO:Defining folds
2025-12-30 11:06:43,488:INFO:Declaring metric variables
2025-12-30 11:06:43,488:INFO:Importing untrained model
2025-12-30 11:06:43,488:INFO:Ridge Classifier Imported successfully
2025-12-30 11:06:43,488:INFO:Starting cross validation
2025-12-30 11:06:43,490:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:43,776:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,779:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,779:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,780:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,782:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,783:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,783:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,785:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

at the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,785:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,787:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,788:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,788:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,789:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,791:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

at the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,793:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:43,794:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,796:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,798:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:43,812:INFO:Calculating mean and std
2025-12-30 11:06:43,812:INFO:Creating metrics dataframe
2025-12-30 11:06:43,814:INFO:Uploading results into container
2025-12-30 11:06:43,814:INFO:Uploading model into container now
2025-12-30 11:06:43,814:INFO:_master_model_container: 6
2025-12-30 11:06:43,814:INFO:_display_container: 2
2025-12-30 11:06:43,815:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-12-30 11:06:43,815:INFO:create_model() successfully completed......................................
2025-12-30 11:06:43,883:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:43,883:INFO:Creating metrics dataframe
2025-12-30 11:06:43,885:INFO:Initializing Random Forest Classifier
2025-12-30 11:06:43,885:INFO:Total runtime is 0.25248136520385744 minutes
2025-12-30 11:06:43,885:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:43,885:INFO:Initializing create_model()
2025-12-30 11:06:43,885:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:43,885:INFO:Checking exceptions
2025-12-30 11:06:43,885:INFO:Importing libraries
2025-12-30 11:06:43,885:INFO:Copying training dataset
2025-12-30 11:06:43,888:INFO:Defining folds
2025-12-30 11:06:43,888:INFO:Declaring metric variables
2025-12-30 11:06:43,888:INFO:Importing untrained model
2025-12-30 11:06:43,888:INFO:Random Forest Classifier Imported successfully
2025-12-30 11:06:43,888:INFO:Starting cross validation
2025-12-30 11:06:43,890:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:44,833:INFO:Calculating mean and std
2025-12-30 11:06:44,842:INFO:Creating metrics dataframe
2025-12-30 11:06:44,847:INFO:Uploading results into container
2025-12-30 11:06:44,848:INFO:Uploading model into container now
2025-12-30 11:06:44,849:INFO:_master_model_container: 7
2025-12-30 11:06:44,849:INFO:_display_container: 2
2025-12-30 11:06:44,850:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-12-30 11:06:44,850:INFO:create_model() successfully completed......................................
2025-12-30 11:06:45,023:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:45,023:INFO:Creating metrics dataframe
2025-12-30 11:06:45,025:INFO:Initializing Quadratic Discriminant Analysis
2025-12-30 11:06:45,025:INFO:Total runtime is 0.27147725820541385 minutes
2025-12-30 11:06:45,025:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:45,025:INFO:Initializing create_model()
2025-12-30 11:06:45,025:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:45,025:INFO:Checking exceptions
2025-12-30 11:06:45,025:INFO:Importing libraries
2025-12-30 11:06:45,025:INFO:Copying training dataset
2025-12-30 11:06:45,028:INFO:Defining folds
2025-12-30 11:06:45,028:INFO:Declaring metric variables
2025-12-30 11:06:45,028:INFO:Importing untrained model
2025-12-30 11:06:45,028:INFO:Quadratic Discriminant Analysis Imported successfully
2025-12-30 11:06:45,028:INFO:Starting cross validation
2025-12-30 11:06:45,031:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:45,276:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,278:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,297:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,305:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,309:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,317:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,322:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,347:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,369:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,370:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,397:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,398:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,406:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,415:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,416:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:45,417:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-12-30 11:06:45,419:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,425:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,440:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,447:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,463:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:45,466:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:45,478:INFO:Calculating mean and std
2025-12-30 11:06:45,479:INFO:Creating metrics dataframe
2025-12-30 11:06:45,482:INFO:Uploading results into container
2025-12-30 11:06:45,482:INFO:Uploading model into container now
2025-12-30 11:06:45,483:INFO:_master_model_container: 8
2025-12-30 11:06:45,483:INFO:_display_container: 2
2025-12-30 11:06:45,483:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-12-30 11:06:45,483:INFO:create_model() successfully completed......................................
2025-12-30 11:06:45,574:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:45,574:INFO:Creating metrics dataframe
2025-12-30 11:06:45,576:INFO:Initializing Ada Boost Classifier
2025-12-30 11:06:45,576:INFO:Total runtime is 0.2806629618008932 minutes
2025-12-30 11:06:45,576:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:45,577:INFO:Initializing create_model()
2025-12-30 11:06:45,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:45,577:INFO:Checking exceptions
2025-12-30 11:06:45,577:INFO:Importing libraries
2025-12-30 11:06:45,577:INFO:Copying training dataset
2025-12-30 11:06:45,581:INFO:Defining folds
2025-12-30 11:06:45,581:INFO:Declaring metric variables
2025-12-30 11:06:45,581:INFO:Importing untrained model
2025-12-30 11:06:45,582:INFO:Ada Boost Classifier Imported successfully
2025-12-30 11:06:45,582:INFO:Starting cross validation
2025-12-30 11:06:45,583:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:45,799:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:45,801:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:45,809:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:45,817:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:45,826:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:45,839:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:45,839:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:45,863:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:45,874:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:45,915:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-12-30 11:06:46,189:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,208:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,224:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,227:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,227:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,231:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,244:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,249:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,257:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,270:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:46,278:INFO:Calculating mean and std
2025-12-30 11:06:46,278:INFO:Creating metrics dataframe
2025-12-30 11:06:46,280:INFO:Uploading results into container
2025-12-30 11:06:46,280:INFO:Uploading model into container now
2025-12-30 11:06:46,280:INFO:_master_model_container: 9
2025-12-30 11:06:46,280:INFO:_display_container: 2
2025-12-30 11:06:46,280:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-12-30 11:06:46,280:INFO:create_model() successfully completed......................................
2025-12-30 11:06:46,369:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:46,369:INFO:Creating metrics dataframe
2025-12-30 11:06:46,373:INFO:Initializing Gradient Boosting Classifier
2025-12-30 11:06:46,373:INFO:Total runtime is 0.29393965800603233 minutes
2025-12-30 11:06:46,373:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:46,373:INFO:Initializing create_model()
2025-12-30 11:06:46,373:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:46,373:INFO:Checking exceptions
2025-12-30 11:06:46,373:INFO:Importing libraries
2025-12-30 11:06:46,373:INFO:Copying training dataset
2025-12-30 11:06:46,376:INFO:Defining folds
2025-12-30 11:06:46,376:INFO:Declaring metric variables
2025-12-30 11:06:46,376:INFO:Importing untrained model
2025-12-30 11:06:46,376:INFO:Gradient Boosting Classifier Imported successfully
2025-12-30 11:06:46,376:INFO:Starting cross validation
2025-12-30 11:06:46,377:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:48,360:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,361:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,363:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,369:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,370:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,373:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,394:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,399:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,415:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,434:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,454:INFO:Calculating mean and std
2025-12-30 11:06:48,455:INFO:Creating metrics dataframe
2025-12-30 11:06:48,457:INFO:Uploading results into container
2025-12-30 11:06:48,458:INFO:Uploading model into container now
2025-12-30 11:06:48,458:INFO:_master_model_container: 10
2025-12-30 11:06:48,459:INFO:_display_container: 2
2025-12-30 11:06:48,459:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-12-30 11:06:48,459:INFO:create_model() successfully completed......................................
2025-12-30 11:06:48,546:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:48,546:INFO:Creating metrics dataframe
2025-12-30 11:06:48,549:INFO:Initializing Linear Discriminant Analysis
2025-12-30 11:06:48,549:INFO:Total runtime is 0.3302108446756999 minutes
2025-12-30 11:06:48,549:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:48,551:INFO:Initializing create_model()
2025-12-30 11:06:48,551:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:48,551:INFO:Checking exceptions
2025-12-30 11:06:48,551:INFO:Importing libraries
2025-12-30 11:06:48,551:INFO:Copying training dataset
2025-12-30 11:06:48,553:INFO:Defining folds
2025-12-30 11:06:48,553:INFO:Declaring metric variables
2025-12-30 11:06:48,553:INFO:Importing untrained model
2025-12-30 11:06:48,554:INFO:Linear Discriminant Analysis Imported successfully
2025-12-30 11:06:48,554:INFO:Starting cross validation
2025-12-30 11:06:48,555:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:48,867:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,870:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,877:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,880:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,885:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,892:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,902:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,905:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,909:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-12-30 11:06:48,929:INFO:Calculating mean and std
2025-12-30 11:06:48,929:INFO:Creating metrics dataframe
2025-12-30 11:06:48,931:INFO:Uploading results into container
2025-12-30 11:06:48,932:INFO:Uploading model into container now
2025-12-30 11:06:48,934:INFO:_master_model_container: 11
2025-12-30 11:06:48,934:INFO:_display_container: 2
2025-12-30 11:06:48,934:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-12-30 11:06:48,934:INFO:create_model() successfully completed......................................
2025-12-30 11:06:49,012:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:49,012:INFO:Creating metrics dataframe
2025-12-30 11:06:49,014:INFO:Initializing Extra Trees Classifier
2025-12-30 11:06:49,014:INFO:Total runtime is 0.33796300093332926 minutes
2025-12-30 11:06:49,014:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:49,014:INFO:Initializing create_model()
2025-12-30 11:06:49,014:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:49,014:INFO:Checking exceptions
2025-12-30 11:06:49,014:INFO:Importing libraries
2025-12-30 11:06:49,014:INFO:Copying training dataset
2025-12-30 11:06:49,018:INFO:Defining folds
2025-12-30 11:06:49,018:INFO:Declaring metric variables
2025-12-30 11:06:49,018:INFO:Importing untrained model
2025-12-30 11:06:49,018:INFO:Extra Trees Classifier Imported successfully
2025-12-30 11:06:49,018:INFO:Starting cross validation
2025-12-30 11:06:49,020:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:49,884:INFO:Calculating mean and std
2025-12-30 11:06:49,885:INFO:Creating metrics dataframe
2025-12-30 11:06:49,887:INFO:Uploading results into container
2025-12-30 11:06:49,887:INFO:Uploading model into container now
2025-12-30 11:06:49,887:INFO:_master_model_container: 12
2025-12-30 11:06:49,887:INFO:_display_container: 2
2025-12-30 11:06:49,888:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-12-30 11:06:49,888:INFO:create_model() successfully completed......................................
2025-12-30 11:06:49,971:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:49,971:INFO:Creating metrics dataframe
2025-12-30 11:06:49,973:INFO:Initializing Light Gradient Boosting Machine
2025-12-30 11:06:49,973:INFO:Total runtime is 0.3539456844329834 minutes
2025-12-30 11:06:49,973:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:49,973:INFO:Initializing create_model()
2025-12-30 11:06:49,973:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:49,973:INFO:Checking exceptions
2025-12-30 11:06:49,973:INFO:Importing libraries
2025-12-30 11:06:49,973:INFO:Copying training dataset
2025-12-30 11:06:49,977:INFO:Defining folds
2025-12-30 11:06:49,977:INFO:Declaring metric variables
2025-12-30 11:06:49,977:INFO:Importing untrained model
2025-12-30 11:06:49,977:INFO:Light Gradient Boosting Machine Imported successfully
2025-12-30 11:06:49,977:INFO:Starting cross validation
2025-12-30 11:06:49,980:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:55,040:INFO:Calculating mean and std
2025-12-30 11:06:55,041:INFO:Creating metrics dataframe
2025-12-30 11:06:55,043:INFO:Uploading results into container
2025-12-30 11:06:55,043:INFO:Uploading model into container now
2025-12-30 11:06:55,043:INFO:_master_model_container: 13
2025-12-30 11:06:55,043:INFO:_display_container: 2
2025-12-30 11:06:55,043:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-12-30 11:06:55,043:INFO:create_model() successfully completed......................................
2025-12-30 11:06:55,134:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:55,134:INFO:Creating metrics dataframe
2025-12-30 11:06:55,136:INFO:Initializing Dummy Classifier
2025-12-30 11:06:55,136:INFO:Total runtime is 0.4399960994720459 minutes
2025-12-30 11:06:55,136:INFO:SubProcess create_model() called ==================================
2025-12-30 11:06:55,136:INFO:Initializing create_model()
2025-12-30 11:06:55,136:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000174A58CC2E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:55,136:INFO:Checking exceptions
2025-12-30 11:06:55,136:INFO:Importing libraries
2025-12-30 11:06:55,136:INFO:Copying training dataset
2025-12-30 11:06:55,140:INFO:Defining folds
2025-12-30 11:06:55,140:INFO:Declaring metric variables
2025-12-30 11:06:55,140:INFO:Importing untrained model
2025-12-30 11:06:55,140:INFO:Dummy Classifier Imported successfully
2025-12-30 11:06:55,140:INFO:Starting cross validation
2025-12-30 11:06:55,141:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-12-30 11:06:55,452:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:55,462:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:55,477:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:55,480:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:55,486:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:55,487:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:55,492:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:55,508:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:55,510:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-12-30 11:06:55,527:INFO:Calculating mean and std
2025-12-30 11:06:55,527:INFO:Creating metrics dataframe
2025-12-30 11:06:55,530:INFO:Uploading results into container
2025-12-30 11:06:55,531:INFO:Uploading model into container now
2025-12-30 11:06:55,532:INFO:_master_model_container: 14
2025-12-30 11:06:55,532:INFO:_display_container: 2
2025-12-30 11:06:55,532:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-12-30 11:06:55,532:INFO:create_model() successfully completed......................................
2025-12-30 11:06:55,613:INFO:SubProcess create_model() end ==================================
2025-12-30 11:06:55,613:INFO:Creating metrics dataframe
2025-12-30 11:06:55,618:WARNING:C:\Users\ENG. Jehad Alef\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-12-30 11:06:55,620:INFO:Initializing create_model()
2025-12-30 11:06:55,621:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000174ECEABF70>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-12-30 11:06:55,621:INFO:Checking exceptions
2025-12-30 11:06:55,621:INFO:Importing libraries
2025-12-30 11:06:55,621:INFO:Copying training dataset
2025-12-30 11:06:55,623:INFO:Defining folds
2025-12-30 11:06:55,623:INFO:Declaring metric variables
2025-12-30 11:06:55,624:INFO:Importing untrained model
2025-12-30 11:06:55,624:INFO:Declaring custom model
2025-12-30 11:06:55,624:INFO:Logistic Regression Imported successfully
2025-12-30 11:06:55,625:INFO:Cross validation set to False
2025-12-30 11:06:55,625:INFO:Fitting Model
2025-12-30 11:06:55,872:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 11:06:55,872:INFO:create_model() successfully completed......................................
2025-12-30 11:06:55,952:INFO:_master_model_container: 14
2025-12-30 11:06:55,952:INFO:_display_container: 2
2025-12-30 11:06:55,952:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-12-30 11:06:55,952:INFO:compare_models() successfully completed......................................
2025-12-30 11:06:56,029:INFO:Initializing save_model()
2025-12-30 11:06:56,030:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=student_performance_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ENG~1.JEH\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Tran...
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-12-30 11:06:56,030:INFO:Adding model into prep_pipe
2025-12-30 11:06:56,038:INFO:student_performance_model.pkl saved in current working directory
2025-12-30 11:06:56,112:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Gender...
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-12-30 11:06:56,112:INFO:save_model() successfully completed......................................
2025-12-30 11:06:56,183:INFO:Soft dependency imported: fastapi: 0.128.0
2025-12-30 11:06:56,183:ERROR:
'uvicorn' is a soft dependency and not included in the pycaret installation. Please run: `pip install uvicorn` to install.
Alternately, you can install this by running `pip install pycaret[mlops]`
NoneType: None
2025-12-30 14:19:09,009:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:19:09,012:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:19:09,012:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:19:09,012:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:19:12,263:INFO:Initializing load_model()
2025-12-30 14:19:12,264:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
2025-12-30 14:22:14,851:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:22:14,852:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:22:14,852:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:22:14,852:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:22:15,915:INFO:Initializing load_model()
2025-12-30 14:22:15,915:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
2025-12-30 14:22:54,950:INFO:Initializing predict_model()
2025-12-30 14:22:54,950:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018431ABFE20>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Ethnicity',
                                             'ParentalEducation', 'Tutoring',
                                             'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering'],
                                    transformer=Simple...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000018431AB36D0>)
2025-12-30 14:22:54,954:INFO:Checking exceptions
2025-12-30 14:22:54,954:INFO:Preloading libraries
2025-12-30 14:22:54,959:INFO:Set up data.
2025-12-30 14:22:54,964:INFO:Set up index.
2025-12-30 14:23:05,121:INFO:Initializing predict_model()
2025-12-30 14:23:05,122:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018431ABDA80>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Ethnicity',
                                             'ParentalEducation', 'Tutoring',
                                             'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering'],
                                    transformer=Simple...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000018431AB36D0>)
2025-12-30 14:23:05,122:INFO:Checking exceptions
2025-12-30 14:23:05,122:INFO:Preloading libraries
2025-12-30 14:23:05,122:INFO:Set up data.
2025-12-30 14:23:05,126:INFO:Set up index.
2025-12-30 14:24:56,358:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:24:56,358:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:24:56,358:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:24:56,358:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:24:59,194:INFO:Initializing load_model()
2025-12-30 14:24:59,194:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
2025-12-30 14:25:03,500:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:25:03,501:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:25:03,502:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:25:03,502:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:25:04,973:INFO:Initializing load_model()
2025-12-30 14:25:04,973:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
2025-12-30 14:26:05,329:INFO:Initializing predict_model()
2025-12-30 14:26:05,329:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B8EE04FEB0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Ethnicity',
                                             'ParentalEducation', 'Tutoring',
                                             'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering'],
                                    transformer=Simple...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002B8EE043640>)
2025-12-30 14:26:05,329:INFO:Checking exceptions
2025-12-30 14:26:05,329:INFO:Preloading libraries
2025-12-30 14:26:05,333:INFO:Set up data.
2025-12-30 14:26:05,339:INFO:Set up index.
2025-12-30 14:26:12,202:INFO:Initializing predict_model()
2025-12-30 14:26:12,202:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B8EE04D8A0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Ethnicity',
                                             'ParentalEducation', 'Tutoring',
                                             'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering'],
                                    transformer=Simple...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002B8EE043640>)
2025-12-30 14:26:12,202:INFO:Checking exceptions
2025-12-30 14:26:12,202:INFO:Preloading libraries
2025-12-30 14:26:12,202:INFO:Set up data.
2025-12-30 14:26:12,206:INFO:Set up index.
2025-12-30 14:30:22,459:INFO:Initializing predict_model()
2025-12-30 14:30:22,460:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B8EE04D8A0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Ethnicity',
                                             'ParentalEducation', 'Tutoring',
                                             'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering'],
                                    transformer=Simple...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002B8EE043640>)
2025-12-30 14:30:22,460:INFO:Checking exceptions
2025-12-30 14:30:22,461:INFO:Preloading libraries
2025-12-30 14:30:22,463:INFO:Set up data.
2025-12-30 14:30:22,472:INFO:Set up index.
2025-12-30 14:31:01,092:INFO:Initializing predict_model()
2025-12-30 14:31:01,092:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B8EE04E380>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Ethnicity',
                                             'ParentalEducation', 'Tutoring',
                                             'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering'],
                                    transformer=Simple...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002B8EE043640>)
2025-12-30 14:31:01,093:INFO:Checking exceptions
2025-12-30 14:31:01,093:INFO:Preloading libraries
2025-12-30 14:31:01,094:INFO:Set up data.
2025-12-30 14:31:01,099:INFO:Set up index.
2025-12-30 14:31:34,214:INFO:Initializing predict_model()
2025-12-30 14:31:34,214:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B8EE04FA30>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Ethnicity',
                                             'ParentalEducation', 'Tutoring',
                                             'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering'],
                                    transformer=Simple...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Ethnicity', 'ParentalEducation',
                                             'ParentalSupport'],
                                    transformer=OneHotEncoder(cols=['Ethnicity',
                                                                    'ParentalEducation',
                                                                    'ParentalSupport'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002B8EE043640>)
2025-12-30 14:31:34,214:INFO:Checking exceptions
2025-12-30 14:31:34,214:INFO:Preloading libraries
2025-12-30 14:31:34,215:INFO:Set up data.
2025-12-30 14:31:34,220:INFO:Set up index.
2025-12-30 14:40:55,149:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:40:55,152:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:40:55,152:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:40:55,153:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:40:58,370:INFO:Initializing load_model()
2025-12-30 14:40:58,370:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
2025-12-30 14:41:04,224:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:41:04,225:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:41:04,225:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:41:04,226:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:41:07,047:INFO:Initializing load_model()
2025-12-30 14:41:07,047:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
2025-12-30 14:55:05,847:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:55:05,848:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:55:05,848:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:55:05,848:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:55:07,066:INFO:Initializing load_model()
2025-12-30 14:55:07,066:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
2025-12-30 14:56:41,824:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:56:41,825:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:56:41,825:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:56:41,825:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-12-30 14:56:43,030:INFO:Initializing load_model()
2025-12-30 14:56:43,030:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
